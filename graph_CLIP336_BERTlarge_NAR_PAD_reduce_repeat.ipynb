{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278d1c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data1 = '20251112_013826'\n",
    "\n",
    "with open('./model/MyOriginal_train_loss_' + data1 + '.csv',encoding='utf-8')as f:\n",
    "    line1_all = f.readlines()\n",
    "\n",
    "with open('./model/MyOriginal_val_loss_' + data1 + '.csv',encoding='utf-8')as f:\n",
    "    line2_all = f.readlines()\n",
    "\n",
    "\n",
    "i = 0\n",
    "x = []\n",
    "loss = []\n",
    "ctc = []\n",
    "kd = []\n",
    "error = []\n",
    "bleu = []\n",
    "v_i = 0\n",
    "v_x = []\n",
    "v_loss = []\n",
    "v_ctc = []\n",
    "v_kd = []\n",
    "v_error = []\n",
    "v_bleu = []\n",
    "\n",
    "#line2 = line2_all[0].replace( \"\\n\", \"\")\n",
    "#len_tr_loader = int(line2)\n",
    "\n",
    "for i, line1 in enumerate( line1_all ):\n",
    "    if i == 0:\n",
    "        line1 = line1.replace( \"\\n\", \"\")\n",
    "        len_tr_loader = int(line1)\n",
    "    else:\n",
    "        line1_split = line1.split(\",\")\n",
    "        i += 1\n",
    "        x.append( i / len_tr_loader )\n",
    "        loss.append( float(line1_split[1].split(' ')[1]) )\n",
    "        ctc.append( float(line1_split[2].split(' ')[1]) )\n",
    "        kd.append( float(line1_split[3].split(' ')[1]) )\n",
    "        error.append( float(line1_split[4].split(' ')[1]) )\n",
    "        bleu.append( float(line1_split[5].split( ' ' )[1]))\n",
    "\n",
    "#print( \"pad:\", pad )\n",
    "\n",
    "for i, line2 in enumerate( line2_all ):\n",
    "    if i == 0:\n",
    "        line2 = line2.replace( \"\\n\", \"\")\n",
    "        len_val_loader = int(line2)\n",
    "    else:\n",
    "        #print( \"line2:\", line2 )\n",
    "        line2_split = line2.split(\",\")\n",
    "        v_i += 1\n",
    "        v_x.append( v_i / len_val_loader )\n",
    "        #v_loss.append( float(line2_split[1].split(' ')[1]) )\n",
    "        #v_ctc.append( float(line2_split[2].split(' ')[1]) )\n",
    "        #v_kd.append( float(line2_split[3].split(' ')[1]) )\n",
    "        v_error.append( float(line2_split[1].split(' ')[1] ) )\n",
    "        v_bleu.append( float(line2_split[2].split( ' ' )[1]))\n",
    "\n",
    "plt.plot( x, loss, label=\"Train loss\")\n",
    "#plt.plot( v_x, v_loss, label=\"Val Loss\" )\n",
    "plt.title( \"Loss\")\n",
    "plt.xlabel( 'Epochs')\n",
    "plt.ylabel( 'Loss')\n",
    "plt.legend()\n",
    "plt.ylim( 0, 1000 )\n",
    "plt.show()\n",
    "\n",
    "plt.plot( x, ctc, label=\"Train crf loss\")\n",
    "#plt.plot( v_x, v_ctc, label=\"Val crf Loss\" )\n",
    "plt.title( \"crf Loss\")\n",
    "plt.xlabel( 'Epochs')\n",
    "plt.ylabel( 'crf Loss')\n",
    "plt.legend()\n",
    "plt.ylim( 0, 600 )\n",
    "plt.show()\n",
    "\n",
    "plt.plot( x, kd, label=\"Train ca loss\")\n",
    "#plt.plot( v_x, v_kd, label=\"Val ca Loss\" )\n",
    "plt.title( \"ca Loss\")\n",
    "plt.xlabel( 'Epochs')\n",
    "plt.ylabel( 'ca Loss')\n",
    "plt.legend()\n",
    "plt.ylim( 0, 200 )\n",
    "plt.show()\n",
    "\n",
    "plt.plot( x, error, label = 'Train WER')\n",
    "plt.plot( v_x, v_error, label=\"Val WER\")\n",
    "plt.title( \"WER\")\n",
    "plt.xlabel( 'Epochs')\n",
    "plt.ylabel( 'WER %')\n",
    "plt.legend()\n",
    "plt.ylim( 60, 120)\n",
    "plt.show()\n",
    "\n",
    "plt.plot( x, bleu, label = 'Train BLEU')\n",
    "plt.plot( v_x, v_bleu, label=\"Val BLEU\")\n",
    "plt.title( \"BLEU\")\n",
    "plt.xlabel( 'Epochs')\n",
    "plt.ylabel( 'BLEU')\n",
    "plt.legend()\n",
    "plt.ylim( 0, 40 )\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa3fae7-5968-4a7b-8583-f927a2b4721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./model/norm_\" + data1 + \".csv\",encoding='utf-8')as f:\n",
    "    line_all = f.readlines()\n",
    "\n",
    "epochs = []\n",
    "steps = []\n",
    "norm0s = []\n",
    "norm1s = []\n",
    "norm_means = []\n",
    "\n",
    "for i, line in enumerate( line_all ):\n",
    "    line_split = line.split( \", \" )\n",
    "    steps.append( float( line_split[1].split(\": \")[1] ) )\n",
    "    epochs.append( float( line_split[1].split(\": \")[1] ) / len_tr_loader )\n",
    "    norm0s.append( float( line_split[2].split(\": \")[1] ) )\n",
    "    norm1s.append( float( line_split[3].split(\": \")[1] ) )\n",
    "    norm_means.append( float( line_split[4].split(\": \")[1] ) )\n",
    "\n",
    "#plt.plot( steps, norm0s, label = 'norm0s')\n",
    "#plt.plot( steps, norm1s, label = 'norm1s')\n",
    "#plt.plot( steps, norm_means, label = 'norm_menas')\n",
    "plt.plot( epochs, norm0s, label = 'norm0s')\n",
    "plt.plot( epochs, norm1s, label = 'norm1s')\n",
    "plt.plot( epochs, norm_means, label = 'norm_menas')\n",
    "plt.title( \"norm\")\n",
    "plt.xlabel( 'steps')\n",
    "plt.ylabel( 'norm')\n",
    "plt.yscale( 'log' )\n",
    "plt.legend()\n",
    "#plt.ylim( 0, 2 )\n",
    "plt.show()\n",
    "\n",
    "#plt.plot( steps, norm0s, label = 'norm0s')\n",
    "plt.plot( epochs, norm0s, label = 'norm0s')\n",
    "plt.title( \"gradients norm\")\n",
    "plt.xlabel( 'steps')\n",
    "plt.ylabel( 'gradients norm')\n",
    "plt.yscale( 'log' )\n",
    "plt.legend()\n",
    "#plt.ylim( 0, 1.0 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b9a7d4-730c-4fab-8c04-09b7b7f6ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_ca( logits, captions, c ):\n",
    "\n",
    "    eps = 1e-4\n",
    "\n",
    "    B, T, V = logits.size()\n",
    "    \n",
    "    one_hot_cap = F.one_hot( captions, num_classes = len( tokenizer ) ) # B * T * V\n",
    "\n",
    "    lcabi = torch.zeros( (B, T),  dtype=torch.float, device = logits.device )\n",
    "    zeroB = torch.zeros( (B),  dtype=torch.float, device = logits.device )\n",
    "    for i in range( T ):\n",
    "        tmp = torch.stack( [ torch.log( (  1.0 - torch.exp( torch.sum( logits[:,i,:] * one_hot_cap[:,j,:], dim = 1 ) ) / \\\n",
    "            ( torch.sum( torch.exp(logits[:,i]), dim = 1 ) + eps ) + eps ) ) if j != i  else zeroB \\\n",
    "            for j in range( max( 0, i - c ), min(  T, i + c ) ) ], dim = 0 )  # window 幅 * B\n",
    "        lcabi[:,i] = torch.sum( tmp, dim = 0 ) # wubdiow 幅 * B を window 幅について sum\n",
    "    \n",
    "    # lcabi は B * T\n",
    "    \n",
    "    pbi = torch.exp( torch.sum( logits * one_hot_cap, dim = 2 ) ) / ( torch.sum( torch.exp( logits ), dim = 2 ) + eps ) # B * T\n",
    "    logpbi = torch.log( pbi ) # B * T\n",
    "    #print( \"logpbi:\", torch.mean( torch.mean( logpbi )))\n",
    "    #logpbi = torch.sum( logits * one_hot_cap, dim = 2 ) - torch.log( torch.sum( torch.exp(logits), dim = 2 ) + eps )\n",
    "    #print( \"logpbi:\", torch.mean( torch.mean(logpbi)) )\n",
    "    \n",
    "    loss_ca = -torch.mean( torch.mean( logpbi + lcabi, dim = 1 ), dim = 0 )\n",
    "    \n",
    "    return loss_ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e63fe93-5891-4b81-a6dd-1386caa9466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel, CLIPVisionModel, BertForPreTraining\n",
    "\n",
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained( model_id )\n",
    "sos_token_id = tokenizer.encode( [ \"[unused0]\" ] )[1]\n",
    "eos_token_id = tokenizer.encode( [ \"[unused1]\" ] )[1]\n",
    "\n",
    "\n",
    "#logits = torch.randn( ( 4, 97, 30522 ) )\n",
    "#captions = torch.randint( 0, len( tokenizer ), size=( 4,97 ) )\n",
    "\n",
    "c = 3\n",
    "\n",
    "loss_ca = calc_loss_ca( logits, captions, c )\n",
    "\n",
    "print( loss_ca )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed48a46a-650a-4f8a-a441-e84130c04872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_ca( logits, captions, c ):\n",
    "\n",
    "    B, T, V = logits.size()\n",
    "    \n",
    "    one_hot_cap = F.one_hot( captions, num_classes = len( tokenizer ) ) # B * T * V\n",
    "\n",
    "    P = torch.exp( torch.sum( logits * one_hot_cap, dim = 2 ) ) / torch.sum( torch.exp( logits ), dim = 2 ) # B * T\n",
    "    loss1 = - torch.sum( torch.sum( torch.log( P ), dim = 1 ) , dim = 0 )\n",
    "\n",
    "    Pca = [ torch.stack( [ torch.exp( torch.sum( logits[:,i,:] * one_hot_cap[:,j,:], dim = 1 ) ) / torch.sum( torch.exp(logits[:,i]), dim = 1 ) \\\n",
    "          for j  in range( T )] , dim = 0 ) for i in range( T ) ]\n",
    "\n",
    "    Pca = torch.stack( Pca, dim = 0 )\n",
    "    Pca = Pca.permute( 2, 1, 0 )\n",
    "\n",
    "    lca = torch.zeros( ( B ), dtype=torch.float, device = logits.device )\n",
    "    zeroB = torch.zeros( (B),  dtype=torch.float, device = logits.device )\n",
    "    for index in range( T ):\n",
    "        tmp = torch.stack( [ torch.log( 1.0 - Pca[:, index, index2 ] ) if index2 != index else zeroB  \\\n",
    "            for index2 in range( max( 0, index - c ), min(  T, index + c ) ) ], dim = 0 )\n",
    "        tmp = tmp.transpose( 0, 1 )\n",
    "        lca[:] += torch.sum( tmp[:], dim = 1 )\n",
    "\n",
    "    lca = - torch.sum( lca , dim = 0 ) * 10000\n",
    "    \n",
    "    print( \"loss1:\", loss1 )\n",
    "    print( \"lca:\", lca )\n",
    "\n",
    "    loss_ca = loss1 + lca\n",
    "    \n",
    "    return loss_ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01725b4-e318-4813-b70c-27a507a3bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel, CLIPVisionModel, BertForPreTraining\n",
    "\n",
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained( model_id )\n",
    "sos_token_id = tokenizer.encode( [ \"[unused0]\" ] )[1]\n",
    "eos_token_id = tokenizer.encode( [ \"[unused1]\" ] )[1]\n",
    "\n",
    "\n",
    "logits = torch.randn( ( 4, 97, 30522 ) )\n",
    "captions = torch.randint( 0, len( tokenizer ), size=( 4,97 ) )\n",
    "\n",
    "B, T, V = logits.size()\n",
    "c = 3\n",
    "\n",
    "loss_ca = calc_loss_ca( logits, captions, c )\n",
    "\n",
    "print( loss_ca )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b399f-1aaa-44fc-acad-54c0029d672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel, CLIPVisionModel, BertForPreTraining\n",
    "\n",
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained( model_id )\n",
    "sos_token_id = tokenizer.encode( [ \"[unused0]\" ] )[1]\n",
    "eos_token_id = tokenizer.encode( [ \"[unused1]\" ] )[1]\n",
    "\n",
    "print( \"sos_tokeni_d:\", sos_token_id )\n",
    "print( \"eos_token_id:\", eos_token_id )\n",
    "\n",
    "logits = torch.randn( ( 4, 97, 30522 ) )\n",
    "captions = torch.randint( 0, len( tokenizer ), size=( 4,97 ) )\n",
    "\n",
    "B, T, V = logits.size()\n",
    "c = 3\n",
    "\n",
    "one_hot_cap = F.one_hot( captions, num_classes = len( tokenizer ) ) # B * T * V\n",
    "\n",
    "#loss_ca = calc_loss_ca( logits, captions, 3 )\n",
    "\n",
    "Pca = [ torch.stack( [ torch.exp( torch.sum( logits[:,i,:] * one_hot_cap[:,j,:], dim = 1 ) ) / torch.sum( torch.exp(logits[:,i]), dim = 1 ) \\\n",
    "          for j  in range( T )] , dim = 0 ) for i in range( T ) ]\n",
    "\n",
    "Pca = torch.stack( Pca, dim = 0 )\n",
    "#Pca = torch.stack( PCa, dim = 1 )\n",
    "\n",
    "Pca = Pca.permute( 2, 1, 0 )\n",
    "print( Pca.size() )\n",
    "\n",
    "\n",
    "lca = torch.zeros( ( B ), dtype=torch.float, device = logits.device )\n",
    "#sum1 = torch.zeros( ( B ), dtype=torch.float, device = logits.device )\n",
    "zeroB = torch.zeros( (B),  dtype=torch.float, device = logits.device )\n",
    "for index in range( T ):\n",
    "    tmp = torch.stack( [ torch.log( 1.0 - Pca[:, index, index2 ] ) if index2 != index else zeroB  \\\n",
    "        for index2 in range( max( 0, index - c ), min(  T, index + c ) ) ], dim = 0 )\n",
    "    tmp = tmp.transpose( 0, 1 )\n",
    "    #sum1[:] = torch.sum( tmp[:], dim = 1 )\n",
    "    #lca[:] += sum1[:]\n",
    "    lca[:] += torch.sum( tmp[:], dim = 1 )\n",
    "\n",
    "#print( lca.size() )   \n",
    "#lca = torch.mean( lca, dim = 0 ) / logits.size(1)\n",
    "#lca = - torch.sum( lca, dim = 0 ) * 10000\n",
    "lca = - torch.sum( lca , dim = 0 ) * 10000\n",
    "\n",
    "print( lca )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12154c3-cdc1-4a15-b8e0-520ef34a9534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel, CLIPVisionModel, BertForPreTraining\n",
    "\n",
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained( model_id )\n",
    "sos_token_id = tokenizer.encode( [ \"[unused0]\" ] )[1]\n",
    "eos_token_id = tokenizer.encode( [ \"[unused1]\" ] )[1]\n",
    "\n",
    "print( \"sos_tokeni_d:\", sos_token_id )\n",
    "print( \"eos_token_id:\", eos_token_id )\n",
    "\n",
    "log_softmax = nn.LogSoftmax( dim = 2 )\n",
    "#softmax = nn.Softmax( dim = 2 )\n",
    "criterion_nll = nn.NLLLoss( reduction = 'none' )\n",
    "\n",
    "log_probs = log_softmax( torch.randn( ( 4, 97, 30522 ) ) )\n",
    "captions = torch.randint( 0, len( tokenizer ), size=( 4,97 ) )\n",
    "\n",
    "def calc_loss_nll( train_log_prob_matrix, train_batch_tgt ):\n",
    "    bsz, tgt_len = train_batch_tgt.size()\n",
    "    \n",
    "    train_log_prob_matrix_unbind = torch.unbind(train_log_prob_matrix, dim = 0)\n",
    "    assert len(train_log_prob_matrix_unbind) == tgt_len\n",
    "    train_truth_tgt_unbind = torch.unbind(train_batch_tgt, dim = 1)\n",
    "    assert len(train_truth_tgt_unbind) == tgt_len\n",
    "\n",
    "    nll_loss_list = []\n",
    "\n",
    "    for index in range(tgt_len):\n",
    "        curr_step_log_prob = train_log_prob_matrix_unbind[index]\n",
    "        curr_step_tgt = train_truth_tgt_unbind[index].view(bsz)\n",
    "        one_nll_loss = criterion_nll(curr_step_log_prob, curr_step_tgt)\n",
    "        #print( \"one_nll_loss size:\", one_nll_loss.size() )\n",
    "        assert one_nll_loss.size() == torch.Size([bsz])\n",
    "        nll_loss_list.append(one_nll_loss)\n",
    "\n",
    "    nll_loss_matrix = torch.stack(nll_loss_list, dim = 1)\n",
    "    #print( \"nll_loss_matrix size:\", nll_loss_matrix.size() )\n",
    "    \n",
    "    train_nll_loss = nll_loss_matrix.sum(-1)\n",
    "    #assert train_batch_crf_loss.size() == train_nll_loss.size()\n",
    "\n",
    "    return train_nll_loss\n",
    "\n",
    "def calc_loss_nll2( train_log_prob_matrix, train_batch_tgt ):\n",
    "    bsz, tgt_len = train_batch_tgt.size()\n",
    "    \n",
    "    train_log_prob_matrix_unbind = torch.unbind(train_log_prob_matrix, dim = 0)\n",
    "    assert len(train_log_prob_matrix_unbind) == tgt_len\n",
    "    train_truth_tgt_unbind = torch.unbind(train_batch_tgt, dim = 1)\n",
    "    assert len(train_truth_tgt_unbind) == tgt_len\n",
    "\n",
    "    nll_loss_list = []\n",
    "\n",
    "    for index in range(tgt_len):\n",
    "        curr_step_log_prob = train_log_prob_matrix_unbind[index]\n",
    "        curr_step_tgt = train_truth_tgt_unbind[index].view(bsz)\n",
    "        one_nll_loss = criterion_nll(curr_step_log_prob, curr_step_tgt)\n",
    "        assert one_nll_loss.size() == torch.Size([bsz])\n",
    "        nll_loss_list.append(one_nll_loss)\n",
    "\n",
    "    nll_loss_matrix = torch.stack(nll_loss_list, dim = 1)\n",
    "\n",
    "    train_nll_loss = nll_loss_matrix.sum(-1)\n",
    "    #assert train_batch_crf_loss.size() == train_nll_loss.size()\n",
    "\n",
    "    return train_nll_loss\n",
    "\n",
    "\n",
    "nll_loss = calc_loss_nll( log_probs.transpose(0,1), captions )\n",
    "print( torch.mean(nll_loss) )\n",
    "\n",
    "\n",
    "print( captions.view(-1).size() )\n",
    "for index in range( log_probs.size(1) ):\n",
    "    nll_loss2 = criterion_nll( log_probs[:,index], captions[:,index] )\n",
    "    #print( nll_loss2 )\n",
    "\n",
    "nll_loss3 = criterion_nll( log_probs.view( log_probs.size(0) * log_probs.size(1), -1 ), captions.view( -1 ) )\n",
    "print( torch.sum( nll_loss3 ) / log_probs.size(0) )\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcc80de-4061-4cd3-81bb-78f183fb1b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
