{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fl7yR7P3CTTj"
   },
   "source": [
    "### ライブラリの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7IQqCE7mCeD"
   },
   "source": [
    "###モジュールのインポートとGoogleドライブのマウント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WLEMbaPJOs9v",
    "outputId": "f47ad633-eb58-49cd-8893-914d53aa4475"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import datetime\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from PIL import Image\n",
    "import skimage.transform\n",
    "from collections import deque\n",
    "from typing import Sequence, Dict, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision import models\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as dataset\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "from transformers import  get_linear_schedule_with_warmup\n",
    "\n",
    "#from transformers import AutoImageProcessor, AutoModel, AutoProcessor, CLIPVisionModel\n",
    "from transformers import BertTokenizer, BertModel, CLIPVisionModel, BertForPreTraining\n",
    "\n",
    "import sys\n",
    "\n",
    "import util\n",
    "import levenshtein\n",
    "from nltk import bleu_score\n",
    "import ssl\n",
    "from torch.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5jwqxtGH7JR"
   },
   "source": [
    "### 位置エンコーディングの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    '''\n",
    "    位置埋め込み （Positional embedding）\n",
    "    dim_embedding: 埋込み次元\n",
    "    max_len      : 入力の最大系列長\n",
    "    '''\n",
    "    def __init__(self, dim_embedding: int, max_len: int=2048):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pos_emb = nn.Embedding(max_len, dim_embedding)\n",
    "\n",
    "    '''\n",
    "    位置エンコーディングの順伝播\n",
    "    x: 位置エンコーディングを埋め込む対象のテンソル,\n",
    "       [バッチサイズ, 系列長, 埋め込み次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        seq = x.shape[1]\n",
    "        positions = torch.arange(start=0, end=seq, step=1, device=x.device).to(torch.long)\n",
    "        positions = self.pos_emb(positions)[:seq,:]\n",
    "        \n",
    "        return positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ja_g99AUIJTF"
   },
   "source": [
    "### Transformerデコーダの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siyThdM4Icll"
   },
   "source": [
    "### CaptioningTransformerの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x, dim=1):\n",
    "    return torch.logsumexp(x.float(), dim=dim).type_as(x)\n",
    "\n",
    "class DynamicCRF(nn.Module):\n",
    "    def __init__(self, num_embedding, low_rank=32, beam_size=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.E1 = nn.Embedding(num_embedding, low_rank)\n",
    "        self.E2 = nn.Embedding(num_embedding, low_rank)\n",
    "\n",
    "        self.vocb = num_embedding\n",
    "        self.rank = low_rank\n",
    "        self.beam = beam_size\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"vocab_size={}, low_rank={}, beam_size={}\".format(\n",
    "            self.vocb, self.rank, self.beam)\n",
    "\n",
    "    def forward(self, emissions, targets, masks, beam=None):\n",
    "        numerator = self._compute_score(emissions, targets, masks)\n",
    "        denominator = self._compute_normalizer(emissions, targets, masks, beam)\n",
    "        return numerator - denominator\n",
    "\n",
    "    def forward_decoder(self, emissions, masks=None, beam=None):\n",
    "        return self._viterbi_decode(emissions, masks, beam)\n",
    "\n",
    "    def _compute_score(self, emissions, targets, masks=None):\n",
    "        batch_size, seq_len = targets.size()\n",
    "        emission_scores = emissions.gather(2, targets[:, :, None])[:, :, 0]  # B x T\n",
    "        transition_scores = (self.E1(targets[:, :-1]) * self.E2(targets[:, 1:])).sum(2)\n",
    "\n",
    "        scores = emission_scores\n",
    "        scores[:, 1:] += transition_scores\n",
    "\n",
    "        if masks is not None:\n",
    "            scores = scores * masks.type_as(scores)\n",
    "        \n",
    "        return scores.sum(-1)\n",
    "\n",
    "    def _compute_normalizer(self, emissions, targets=None, masks=None, beam=None):\n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = emissions.size()[:2]\n",
    "        if targets is not None:\n",
    "            #_emissions = emissions.scatter(2, targets[:, :, None], np.float('inf'))\n",
    "            _emissions = emissions.scatter(2, targets[:, :, None], float('inf'))\n",
    "            beam_targets = _emissions.topk(beam, 2)[1]\n",
    "            beam_emission_scores = emissions.gather(2, beam_targets)\n",
    "        else:\n",
    "            beam_emission_scores, beam_targets = emissions.topk(beam, 2)\n",
    "        beam_transition_score1 = self.E1(beam_targets[:, :-1])  # B x (T-1) x K x D; position i - 1, previous step.\n",
    "        beam_transition_score2 = self.E2(beam_targets[:, 1:])   # B x (T-1) x K x D; position i, current step.\n",
    "        beam_transition_matrix = torch.bmm(\n",
    "            beam_transition_score1.view(-1, beam, self.rank),\n",
    "            beam_transition_score2.view(-1, beam, self.rank).transpose(1, 2))\n",
    "        beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam)\n",
    "\n",
    "        # compute the normalizer in the log-space\n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        for i in range(1, seq_len):\n",
    "            next_score = score[:, :, None] + beam_transition_matrix[:, i-1]\n",
    "            next_score = logsumexp(next_score, dim=1) + beam_emission_scores[:, i]\n",
    "\n",
    "            if masks is not None:\n",
    "                score = torch.where(masks[:, i:i+1], next_score, score)\n",
    "            else:\n",
    "                score = next_score\n",
    "\n",
    "        # Sum (log-sum-exp) over all possible tags\n",
    "        return logsumexp(score, dim=1)\n",
    "\n",
    "    def _viterbi_decode(self, emissions, masks=None, beam=None):\n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = emissions.size()[:2]\n",
    "        beam_emission_scores, beam_targets = emissions.topk(beam, 2)\n",
    "        beam_transition_score1 = self.E1(beam_targets[:, :-1])  # B x (T-1) x K x D\n",
    "        beam_transition_score2 = self.E2(beam_targets[:, 1:])   # B x (T-1) x K x D\n",
    "        beam_transition_matrix = torch.bmm(\n",
    "            beam_transition_score1.view(-1, beam, self.rank),\n",
    "            beam_transition_score2.view(-1, beam, self.rank).transpose(1, 2))\n",
    "        beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam)\n",
    "\n",
    "        traj_tokens, traj_scores = [], []\n",
    "        finalized_tokens, finalized_scores = [], []\n",
    "\n",
    "        # compute the normalizer in the log-space\n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        #print( \"score size:\", score.size() )\n",
    "        dummy = torch.arange(beam, device=score.device).expand(*score.size()).contiguous()\n",
    "\n",
    "        for i in range(1, seq_len):\n",
    "            traj_scores.append(score)\n",
    "            _score = score[:, :, None] + beam_transition_matrix[:, i-1]\n",
    "            _score, _index = _score.max(dim=1)\n",
    "            _score = _score + beam_emission_scores[:, i]\n",
    "\n",
    "            if masks is not None:\n",
    "                score = torch.where(masks[:, i: i+1], _score, score)\n",
    "                index = torch.where(masks[:, i: i+1], _index, dummy)\n",
    "            else:\n",
    "                score, index = _score, _index\n",
    "            traj_tokens.append(index)\n",
    "\n",
    "        # now running the back-tracing and find the best\n",
    "        best_score, best_index = score.max(dim=1)\n",
    "        finalized_tokens.append(best_index[:, None])\n",
    "        finalized_scores.append(best_score[:, None])\n",
    "\n",
    "        for idx, scs in zip(reversed(traj_tokens), reversed(traj_scores)):\n",
    "            previous_index = finalized_tokens[-1]\n",
    "            finalized_tokens.append(idx.gather(1, previous_index))\n",
    "            finalized_scores.append(scs.gather(1, previous_index))\n",
    "\n",
    "        finalized_tokens.reverse()\n",
    "        finalized_tokens = torch.cat(finalized_tokens, 1)\n",
    "        finalized_tokens = beam_targets.gather(2, finalized_tokens[:, :, None])[:, :, 0]\n",
    "\n",
    "        finalized_scores.reverse()\n",
    "        finalized_scores = torch.cat(finalized_scores, 1)\n",
    "        finalized_scores[:, 1:] = finalized_scores[:, 1:] - finalized_scores[:, :-1]\n",
    "\n",
    "        return finalized_scores, finalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, crf_low_rank, crf_beam_size, dropout, padding_idx):\n",
    "        super(TopLayer, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "        self.padding_idx = padding_idx\n",
    "        print( \"in TopLyaer:\" )\n",
    "        self.crf_layer = DynamicCRF(num_embedding = vocab_size, low_rank = crf_low_rank, \n",
    "                                    beam_size = crf_beam_size)\n",
    "\n",
    "        #self.one_more_layer_norm = nn.LayerNorm(embed_dim)\n",
    "        #self.tgt_word_prj = nn.Linear(self.embed_dim, self.vocab_size)\n",
    "\n",
    "    def forward(self, src_representation, src_input, tgt_input, is_training):\n",
    "        '''\n",
    "            src_representation : bsz x seqlen x embed_dim\n",
    "            src_input : bsz x seqlen\n",
    "            tgt_input : bsz x seqlen\n",
    "        '''\n",
    "        #assert src_input.size() == tgt_input.size()\n",
    "\n",
    "        src_input = src_input.transpose(0, 1) # src_len x bsz\n",
    "        #seqlen, bsz = src_input.size()\n",
    "        seqlen, bsz = src_input.shape[:2]\n",
    "\n",
    "        src_representation = F.dropout(src_representation, p=self.dropout, training=is_training)\n",
    "        src_representation = src_representation.transpose(0, 1) # seqlen x bsz x embed_dim\n",
    "\n",
    "        #src = src_representation\n",
    "\n",
    "        #emissions = self.tgt_word_prj(src.contiguous().view(-1, self.embed_dim)).view(seqlen, bsz, self.vocab_size)\n",
    "        emissions = src_representation\n",
    "        #log_probs = torch.log_softmax(emissions, -1)\n",
    "        #assert log_probs.size() == torch.Size([seqlen, bsz, self.vocab_size])\n",
    "\n",
    "        emissions = emissions.transpose(0, 1) # [bsz x src_len x vocab_size]\n",
    "        #emission_mask = ~tgt_input.eq(self.padding_idx) # [bsz x src_len] #pad のところは 0 padでないところが 1\n",
    "        emission_mask = torch.ones_like( tgt_input, dtype=torch.bool ) #全部　pad でないとして 1\n",
    "        batch_crf_loss = -1 * self.crf_layer(emissions, tgt_input, emission_mask) # [bsz]\n",
    "        assert batch_crf_loss.size() == torch.Size([bsz])\n",
    "        #return log_probs, batch_crf_loss\n",
    "        #return torch.mean( batch_crf_loss )\n",
    "        #print( \"batch_crf_loss:\", batch_crf_loss )\n",
    "        return batch_crf_loss\n",
    "\n",
    "    def decoding(self, src_representation, src_input):\n",
    "        '''\n",
    "            src_representation : bsz x seqlen x embed_dim\n",
    "            src_input : bsz x seqlen\n",
    "            tgt_input : bsz x seqlen\n",
    "        '''\n",
    "        src_input = src_input.transpose(0, 1) # src_len x bsz\n",
    "        seqlen, bsz = src_input.size()\n",
    "\n",
    "        src_representation = src_representation.transpose(0, 1) # seqlen x bsz x embed_dim\n",
    "        src = src_representation\n",
    "\n",
    "        emissions = self.tgt_word_prj(src.contiguous().view(-1, self.embed_dim)).view(seqlen, bsz, self.vocab_size)\n",
    "\n",
    "        emissions = emissions.transpose(0, 1) # [bsz, seqlen, vocab_size]\n",
    "        _, finalized_tokens = self.crf_layer.forward_decoder(emissions)\n",
    "        assert finalized_tokens.size() == torch.Size([bsz, seqlen])\n",
    "        return finalized_tokens\n",
    "\n",
    "    def length_ratio_decoding(self, src_representation, src_input, length_ratio):\n",
    "        '''\n",
    "            src_representation : 1 x seqlen x embed_dim\n",
    "            src_input : 1 x seqlen\n",
    "        '''\n",
    "        src_input = src_input.transpose(0, 1) # src_len x bsz\n",
    "        seqlen, bsz = src_input.size()\n",
    "\n",
    "        src_representation = src_representation.transpose(0, 1) # seqlen x bsz x embed_dim\n",
    "        src = src_representation\n",
    "\n",
    "        emissions = self.tgt_word_prj(src.contiguous().view(-1, self.embed_dim)).view(seqlen, bsz, self.vocab_size)\n",
    "\n",
    "        emissions = emissions.transpose(0, 1) # [bsz, seqlen, vocab_size]\n",
    "        valid_len = int(seqlen * length_ratio) + 1\n",
    "        valid_emissions = emissions[:, :valid_len+1,:]\n",
    "        _, finalized_tokens = self.crf_layer.forward_decoder(valid_emissions)\n",
    "        return finalized_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HcmR9lKrIbiL"
   },
   "outputs": [],
   "source": [
    "class CaptioningTransformer(nn.Module):\n",
    "    '''\n",
    "    CaptioningTransformerのコンストラクタ\n",
    "    dim_embedding  : 埋め込み次元\n",
    "    dim_feedforward: FNNの中間特徴次元\n",
    "    num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    num_layers     : Transformerデコーダ層の数\n",
    "    vocab_size     : 辞書の次元\n",
    "    null_index     : NULLのID\n",
    "    dropout        : ドロップアウト確率\n",
    "    '''\n",
    "    def __init__(self, img_size: int, dim_embedding: int, length_max: int,\n",
    "                  vocab_size: int, tokenizer, dropout: float=0.1, model_id: str=''):\n",
    "        super().__init__()\n",
    "\n",
    "        #CLIP\n",
    "        clip_model_id = \"openai/clip-vit-large-patch14-336\"\n",
    "        self.clip_model = CLIPVisionModel.from_pretrained(clip_model_id, output_hidden_states = True)\n",
    "        images = torch.randn( ( 1, 3, img_size, img_size ) )\n",
    "        memory = self.clip_model( images )\n",
    "        memory = memory.last_hidden_state\n",
    "        img_length = memory.size(1)\n",
    "        clip_dim = memory.size(2)\n",
    "        \n",
    "        # Dense Connector\n",
    "        self.dc_linear = nn.Linear( clip_dim * 3, dim_embedding )\n",
    "        self.dropout = nn.Dropout( dropout )        \n",
    "        self.ln_memory = nn.LayerNorm( dim_embedding )\n",
    "\n",
    "        # Down Sampling\n",
    "        stride = img_length // ( length_max - 1 )\n",
    "        self.conv1 = nn.Conv1d( dim_embedding, dim_embedding, 1, stride )\n",
    "        print( \"img_length:\", img_length )\n",
    "        print( \"text_length_max:\", length_max )\n",
    "        print( \"stride:\", stride )\n",
    "        memory = self.conv1( memory.transpose(1,2) ).transpose(1,2)\n",
    "        print( \"bert in memory size:\", memory.size() )\n",
    "\n",
    "        self.pos_emb = PositionalEmbedding( dim_embedding )\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained( model_id )\n",
    "\n",
    "        ## 単語出力分布計算\n",
    "        self.ln_outputs = nn.LayerNorm( dim_embedding )\n",
    "        self.linear = nn.Linear(dim_embedding, vocab_size)\n",
    "\n",
    "        crf_low_rank = 32\n",
    "        crf_beam_size = 256\n",
    "        top_dropout = 0.0\n",
    "        tgt_padding_idx = tokenizer.pad_token_id\n",
    "        print( \"initialize self.toplayer\" )\n",
    "        self.toplayer = TopLayer( vocab_size, dim_embedding, crf_low_rank, crf_beam_size, top_dropout, tgt_padding_idx )\n",
    "        \n",
    "        self.dim_embedding = dim_embedding\n",
    "\n",
    "    ''' CaptioningTransformerの順伝播処理\n",
    "    features: 画像特徴量 [バッチサイズ, 埋め込み次元]\n",
    "    captions: 正解キャプション [バッチサイズ, 系列長]\n",
    "    '''\n",
    "    def forward(self, images: torch.Tensor ):\n",
    "\n",
    "        self.device = images.device\n",
    "\n",
    "        memory = self.clip_model( images )\n",
    "        memory = self.dense_connector( memory )\n",
    "        memory = self.dropout( memory )\n",
    "        memory = self.ln_memory( memory )\n",
    "\n",
    "        memory = self.conv1( memory.transpose(1,2) ).transpose(1,2)\n",
    "\n",
    "        memory += self.pos_emb( memory )\n",
    "        \n",
    "        outputs = self.bert( inputs_embeds = memory ).last_hidden_state\n",
    "        outputs = self.ln_outputs( outputs )\n",
    "        logits = self.linear( outputs )\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    def inference(self, images: torch.Tensor ):\n",
    "\n",
    "        self.device = images.device\n",
    "\n",
    "        memory = self.clip_model( images )\n",
    "        memory = self.dense_connector( memory )\n",
    "        memory = self.dropout( memory )\n",
    "        memory = self.ln_memory( memory )\n",
    "\n",
    "        memory = self.conv1( memory.transpose(1,2) ).transpose(1,2)\n",
    "\n",
    "        memory += self.pos_emb( memory )\n",
    "        \n",
    "        outputs = self.bert( inputs_embeds = memory ).last_hidden_state\n",
    "        outputs = self.ln_outputs( outputs )\n",
    "        emissions = self.linear( outputs )\n",
    "\n",
    "        #emissions = emissions.transpose(0, 1) # [bsz, seqlen, vocab_size]\n",
    "        seqlen = emissions.size(1) #added by T.uchi\n",
    "        length_ratio = 1.0         #added by T.uchi\n",
    "        #length_ratio = 0.7         #added by T.uchi\n",
    "        valid_len = int(seqlen * length_ratio) + 1\n",
    "        valid_emissions = emissions[:, :valid_len+1,:]\n",
    "        _, finalized_tokens = self.toplayer.crf_layer.forward_decoder(valid_emissions)\n",
    "        return finalized_tokens\n",
    "\n",
    "    def dense_connector(self, memory ):\n",
    "        tmp1 = torch.tensor([], device = self.device )\n",
    "        tmp2 = torch.tensor([], device = self.device )\n",
    "        tmp_full = len( memory.hidden_states )\n",
    "        tmp_half = tmp_full // 2\n",
    "        for i in range( 0, tmp_half ):\n",
    "            tmp1 = torch.cat( [tmp1, memory.hidden_states[i][None]], dim = 0 )\n",
    "        tmp1 = torch.sum(tmp1, dim=0) / tmp_half\n",
    "        for i in range( tmp_half, tmp_full ):\n",
    "            tmp2 = torch.cat( [tmp2, memory.hidden_states[i][None]], dim = 0 )\n",
    "        tmp2 = torch.sum(tmp2, dim=0 ) / ( tmp_full - tmp_half )\n",
    "        tmp3 = torch.cat([tmp1, tmp2], dim=-1)\n",
    "        tmp3 = torch.cat( [ memory.last_hidden_state, tmp3], dim = -1 )\n",
    "        tmp3 = self.dc_linear( tmp3 )\n",
    "        return tmp3\n",
    "\n",
    "    def my_decode(self, token_list, tokenizer ):\n",
    "\n",
    "        def my_index( l, x ):\n",
    "            if x in l:\n",
    "                return l.index(x)\n",
    "            else:\n",
    "                return -1\n",
    "        if my_index( token_list, eos_token_id ) != -1:\n",
    "            token_list = token_list[:my_index( token_list, eos_token_id )]\n",
    "        else:\n",
    "            token_list = token_list\n",
    "            \n",
    "        text = tokenizer.decode( token_list, skip_special_tokens = True )\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_path: str, img_directory: str, transforms, tokenizer, length_max = None ) -> None:\n",
    "        super().__init__()\n",
    "        self.img_directory = img_directory\n",
    "        self.transforms = transforms\n",
    "        # TODO: fix to original data\n",
    "        #画像の前処理\n",
    "        self.img_file = []\n",
    "        self.tokens = []\n",
    "        #vocab_size = len( tokenizer )\n",
    "        #c1 = torch.zeros( ( vocab_size ) )\n",
    "        #c2 = torch.zeros( ( vocab_size, vocab_size ) )\n",
    "        if length_max == None:\n",
    "            self.length_max = 0\n",
    "        else:\n",
    "            self.length_max = length_max\n",
    "        length_sum = 0\n",
    "\n",
    "        with open( file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        for i, line_data in enumerate( data ):\n",
    "            if i % 100000 == 0:\n",
    "                print( \"i:\", i )\n",
    "            self.img_file.append( line_data['img_file'] )\n",
    "            id_tokens = line_data['id_tokens']\n",
    "            id_tokens.append( eos_token_id )\n",
    "            id_tokens.append( eos_token_id )\n",
    "            length_sum += len( id_tokens )\n",
    "            if length_max != None:\n",
    "                id_tokens = torch.tensor( id_tokens )[:self.length_max]\n",
    "            else:\n",
    "                if self.length_max < len( id_tokens ):\n",
    "                    self.length_max = len( id_tokens )\n",
    "                id_tokens = torch.tensor( id_tokens )\n",
    "            self.tokens.append( id_tokens )\n",
    "        # w1, w2 を作る時は length_max = None　でお願いします。\n",
    "        #    for i2 in range( len(id_tokens) ):\n",
    "        #        if i2 == len( id_tokens ) - 1:\n",
    "        #            c1[id_tokens[i2]] += 1\n",
    "        #        else:\n",
    "        #            c1[id_tokens[i2]] += 1\n",
    "        #            c2[id_tokens[i2], id_tokens[i2+1] ] += 1\n",
    "        '''\n",
    "        c1avg = int( torch.sum( c1 ) / torch.sum( torch.ne( c1, 0 ).int()) )\n",
    "        c2avg = int( torch.sum( torch.sum( c2, dim = 1 ), dim = 0 ) / torch.sum( torch.ne( c2, 0 ).int() ) )\n",
    "\n",
    "        c1[0] = c1avg\n",
    "\n",
    "        c2[:,0] = c2avg\n",
    "        c2[0,:] = c2avg\n",
    "        \n",
    "        sumc1 = torch.sum( c1, dim = 0 )\n",
    "        sumc2 = torch.sum( torch.sum( c2, dim = 1 ), dim = 0 )\n",
    "\n",
    "        prob1 = c1 / sumc1\n",
    "        prob2 = c2 / sumc2\n",
    "\n",
    "        self.w1 = prob1 ** -0.4\n",
    "        self.w1 = torch.nan_to_num( self.w1, nan = 0.0, posinf=0.0, neginf=0.0 )\n",
    "        avg1 = torch.sum( self.w1, dim = 0 ) / torch.sum( torch.ne( self.w1, 0.0 ).int() )\n",
    "        self.w1 = self.w1 / avg1\n",
    "\n",
    "        self.w2 = prob2 ** -0.4\n",
    "        self.w2 = torch.nan_to_num( self.w2, nan = 0.0, posinf=0.0, neginf=0.0 )\n",
    "        avg2 = torch.sum( torch.sum( self.w2, dim = 1 ), dim = 0 ) / torch.sum( torch.ne( self.w2, 0.0 ).int() )\n",
    "        self.w2 = self.w2 / avg2\n",
    "\n",
    "        with open( \"/mnt/ssd2/v7/w_unigrma.pkl\", mode=\"wb\" ) as f:\n",
    "            pickle.dump( self.w1, f )\n",
    "\n",
    "        with open( \"/mnt/ssd2/v7/w_bigrma.pkl\", mode=\"wb\" ) as f:\n",
    "            pickle.dump( self.w2, f )\n",
    "        \n",
    "        '''\n",
    "\n",
    "        #with open( \"/mnt/ssd2/v7/w_unigram.pkl\", 'rb') as f:\n",
    "        #    self.w1 = pickle.load(f)\n",
    "\n",
    "        #with open( \"/mnt/ssd2/v7/w_bigram.pkl\", 'rb') as f:\n",
    "        #    self.w2 = pickle.load(f)\n",
    "        \n",
    "        if length_max == None:\n",
    "            print( \"length max:\", self.length_max )\n",
    "            print( \"avg length:\", length_sum / len( self.tokens ) )\n",
    "    \n",
    "    # ここで取り出すデータを指定している\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        index: int\n",
    "    ):\n",
    "        tokens = self.tokens[index]\n",
    "        img_file = self.img_file[index] + \".jpg\"\n",
    "        img_path = os.path.join( self.img_directory, img_file ) #index番目の画像のパスを取得\n",
    "        img = Image.open(img_path) #PIL形式で画像を読み込み\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert(\"RGB\")\n",
    "        img = self.transforms(img)\n",
    "        \n",
    "        return img, tokens\n",
    "\n",
    "    # この method がないと DataLoader を呼び出す際にエラーを吐かれる\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def length_max(self):\n",
    "        return self.length_max\n",
    "\n",
    "    #def w1(self):\n",
    "    #    return self.w1\n",
    "\n",
    "    #def w2(self):\n",
    "    #    return self.w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_func(batch: Sequence[Tuple[Union[torch.Tensor, str]]], pad_index, length_max ):\n",
    "    imgs, tokens = zip(*batch)\n",
    "\n",
    "    max_length = length_max\n",
    "    #max_length = 0\n",
    "    #for target in tokens:\n",
    "    #    if max_length < len( target ):\n",
    "    #        max_length = len( target )\n",
    "    \n",
    "    targets = []\n",
    "    lengths = []\n",
    "    for target in tokens:\n",
    "        pad_len = max_length - len( target ) \n",
    "        #print( \"target:\", target )\n",
    "        input2= F.pad( target, (0, pad_len), mode='constant', value = pad_index)\n",
    "        targets.append( input2 )\n",
    "        lengths.append( len( target ) )\n",
    "    \n",
    "    imgs = torch.stack( imgs, dim = 0 )\n",
    "    targets = torch.stack( targets, dim = 0 )\n",
    "    lengths = torch.tensor( lengths, requires_grad = False  )\n",
    "\n",
    "    #if imgs.dim() != 4:\n",
    "    #   print( \"in collate imgs size:\", imgs.size() )\n",
    "    \n",
    "    return imgs, targets, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4x-PO05mCS-"
   },
   "source": [
    "###学習におけるハイパーパラメータやオプションの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EQES3A8OG-V_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sos_tokeni_d: 1\n",
      "eos_token_id: 2\n"
     ]
    }
   ],
   "source": [
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained( model_id )\n",
    "sos_token_id = tokenizer.encode( [ \"[unused0]\" ] )[1]\n",
    "eos_token_id = tokenizer.encode( [ \"[unused1]\" ] )[1]\n",
    "\n",
    "print( \"sos_tokeni_d:\", sos_token_id )\n",
    "print( \"eos_token_id:\", eos_token_id )\n",
    "\n",
    "class ConfigTrain(object):\n",
    "    '''\n",
    "    ハイパーパラメータ、システム共通変数の設定\n",
    "    '''\n",
    "    def __init__(self):\n",
    "\n",
    "        # ハイパーパラメータ\n",
    "        self.img_size = 336\n",
    "        self.dim_embedding = 1024   # 埋め込み層の次元\n",
    "        self.length_max = 97\n",
    "        #self.lr = 5e-5            # 学習率\n",
    "        #self.lr = 2e-5            # 学習率\n",
    "        self.lr_clip = 2e-7\n",
    "        self.lr_bert = 2e-5            # 学習率\n",
    "        self.lr_others = 4e-5\n",
    "        #self.lr_top = 1e-4\n",
    "        #self.lr = 5e-6            # 学習率\n",
    "        self.dropout = 0.1         # dropout確率\n",
    "        #self.batch_size = 128       # ミニバッチ数\n",
    "        self.batch_size = 32       # ミニバッチ数\n",
    "        #self.batch_size = 16       # ミニバッチ数\n",
    "        #self.batch_size = 8       # ミニバッチ数\n",
    "        #self.batch_size = 4       # ミニバッチ数\n",
    "        #self.batch_size = 1       # ミニバッチ数\n",
    "        #self.num_epochs = 100       # エポック数→Colab無料版でテストする際は10未満に修正を推奨\n",
    "        #self.num_epochs = 100       # エポック数→Colab無料版でテストする際は10未満に修正を推奨\n",
    "        #self.num_epochs = 60       # エポック数→Colab無料版でテストする際は10未満に修正を推奨\n",
    "        self.num_epochs = 10       # エポック数→Colab無料版でテストする際は10未満に修正を推奨\n",
    "        self.use_amp = True\n",
    "        #self.use_amp = False\n",
    "        #self.use_saved_pth = True\n",
    "        self.use_saved_pth = False\n",
    "        self.model_id = \"google-bert/bert-large-uncased\"\n",
    "        self.vocab_size = len( tokenizer )\n",
    "        self.weight_decay = 0.1\n",
    "        self.betas = (0.9, 0.999 )\n",
    "        self.warmup = 0.1\n",
    "        #self.alpha = 1.0\n",
    "        self.alpha = 0.5\n",
    "        self.window_size = 3\n",
    "        \n",
    "        # パスの設定\n",
    "        self.img_directory = '/mnt/ssd2/v7/img'\n",
    "        self.anno_file = '/mnt/ssd2/v7/data.pkl'\n",
    "        self.save_directory = './model'\n",
    "\n",
    "        # 検証に使う学習セット内のデータの割合\n",
    "        self.test_ratio = 0.1\n",
    "        self.val_ratio = 0.1\n",
    "        #self.val_ratio = 0.0004\n",
    "        #self.test_ratio = 0.0004\n",
    "        \n",
    "        # 学習に使うデバイス\n",
    "        #self.device = 'cuda'\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #self.device = 'cpu'\n",
    "        \n",
    "        # データローダーに使うCPUプロセスの数\n",
    "        #self.num_workers = 4\n",
    "        self.num_workers = 0 if self.device == torch.device('cpu') else 12\n",
    "        #self.num_workers = 0\n",
    "        \n",
    "        # 移動平均で計算する損失の値の数\n",
    "        self.moving_avg = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_length: 577\n",
      "text_length_max: 97\n",
      "stride: 6\n",
      "bert in memory size: torch.Size([1, 97, 1024])\n",
      "initialize self.toplayer\n",
      "in TopLyaer:\n",
      "torch.Size([1, 97, 30522])\n"
     ]
    }
   ],
   "source": [
    "#config = \"\"\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "## 辞書（単語→単語ID）の読み込み\n",
    "#with open('../PreTrain_Decoder/translateDatasetNTT_blank4_pad0/word_to_id2.pkl', 'rb') as f:\n",
    "#    word_to_id = pickle.load(f)\n",
    "#max_idx_en = len( word_to_id )\n",
    "#word_to_id['<mask>'] = max_idx_en\n",
    "#mask_value = word_to_id['<mask>']\n",
    "#start_idx = word_to_id['<start>']\n",
    "#bert_model_path = 'models--google-bert--bert-large-uncased/snapshots/6da4b6a26a1877e173fca3225479512db81a5e5b'\n",
    "#tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path = bert_model_path )\n",
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id)\n",
    "model = CaptioningTransformer( img_size = 336, dim_embedding=1024, length_max = 97, vocab_size=len(tokenizer),\n",
    "                 tokenizer=tokenizer, dropout=0.1, model_id =model_id).to(device)\n",
    "\n",
    "#images = torch.randint( 0, 255, size = (10,3,256,256) )\n",
    "images = torch.randn( ( 1, 3, 336,336 ) )\n",
    "outputs = model( images )\n",
    "\n",
    "print( outputs.size() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbR7QGrr5ouJ"
   },
   "source": [
    "### 学習率スケジューラ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--NNWCwZI5qS"
   },
   "source": [
    "### 学習を行う関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_nll( train_log_prob_matrix, train_batch_tgt ):\n",
    "    bsz, tgt_len = train_batch_tgt.size()\n",
    "    \n",
    "    train_log_prob_matrix_unbind = torch.unbind(train_log_prob_matrix, dim = 0)\n",
    "    assert len(train_log_prob_matrix_unbind) == tgt_len\n",
    "    train_truth_tgt_unbind = torch.unbind(train_batch_tgt, dim = 1)\n",
    "    assert len(train_truth_tgt_unbind) == tgt_len\n",
    "\n",
    "    nll_loss_list = []\n",
    "\n",
    "    for index in range(tgt_len):\n",
    "        curr_step_log_prob = train_log_prob_matrix_unbind[index]\n",
    "        curr_step_tgt = train_truth_tgt_unbind[index].view(bsz)\n",
    "        #one_nll_loss = NLL(curr_step_log_prob, curr_step_tgt)\n",
    "        one_nll_loss = criterion_nll(curr_step_log_prob, curr_step_tgt)\n",
    "        assert one_nll_loss.size() == torch.Size([bsz])\n",
    "        nll_loss_list.append(one_nll_loss)\n",
    "\n",
    "    nll_loss_matrix = torch.stack(nll_loss_list, dim = 1)\n",
    "    #assert nll_loss_matrix.size() == torch.Size([bsz, tgt_len])\n",
    "    #tgt_padding_matrix = ~train_batch_tgt.eq(data.tgt_padding_idx)\n",
    "    #tgt_padding_matrix = tgt_padding_matrix.type(nll_loss_matrix.type())\n",
    "    #assert tgt_padding_matrix.size() == nll_loss_matrix.size()\n",
    "    #nll_loss_matrix = nll_loss_matrix * tgt_padding_matrix\n",
    "\n",
    "    train_nll_loss = nll_loss_matrix.sum(-1)\n",
    "    #assert train_batch_crf_loss.size() == train_nll_loss.size()\n",
    "\n",
    "    return train_nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def calc_loss_ca( logits, captions, c ):\n",
    "\n",
    "    eps = 1e-4\n",
    "\n",
    "    B, T, V = logits.size()\n",
    "    \n",
    "    one_hot_cap = F.one_hot( captions, num_classes = len( tokenizer ) ) # B * T * V\n",
    "\n",
    "    lcabi = torch.zeros( (B, T),  dtype=torch.float, device = logits.device )\n",
    "    zeroB = torch.zeros( (B),  dtype=torch.float, device = logits.device )\n",
    "    for i in range( T ):\n",
    "        tmp = torch.stack( [ torch.log( (  1.0 - torch.exp( torch.sum( logits[:,i,:] * one_hot_cap[:,j,:], dim = 1 ) ) / \\\n",
    "            ( torch.sum( torch.exp(logits[:,i]), dim = 1 ) + eps ) + eps ) ) if j != i  else zeroB \\\n",
    "            for j in range( max( 0, i - c ), min(  T, i + c ) ) ], dim = 0 )  # window 幅 * B\n",
    "        lcabi[:,i] = torch.sum( tmp, dim = 0 ) # wubdiow 幅 * B を window 幅について sum\n",
    "    \n",
    "    # lcabi は B * T\n",
    "    \n",
    "    #pbi = torch.exp( torch.sum( logits * one_hot_cap, dim = 2 ) ) / ( torch.sum( torch.exp( logits ), dim = 2 ) + eps ) # B * T\n",
    "    #logpbi = torch.log( pbi ) # B * T\n",
    "    #print( \"logpbi:\", torch.mean( torch.mean( logpbi )))\n",
    "    logpbi = torch.sum( logits * one_hot_cap, dim = 2 ) - torch.log( torch.sum( torch.exp(logits), dim = 2 ) + eps ) # B * T\n",
    "    #print( \"logpbi:\", torch.mean( torch.mean(logpbi)) )\n",
    "    \n",
    "    loss_ca = -torch.mean( torch.mean( logpbi + lcabi, dim = 1 ), dim = 0 )\n",
    "    \n",
    "    return loss_ca\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8275)\n"
     ]
    }
   ],
   "source": [
    "logits = torch.randn( ( 4, 97, 30522 ) )\n",
    "captions = torch.randint( 0, len( tokenizer ), size=( 4,97 ) )\n",
    "\n",
    "loss_ca = calc_loss_ca( logits, captions, 3 )\n",
    "\n",
    "print( loss_ca )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xBOP-3aIHFjB",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n",
      "i: 100000\n",
      "i: 200000\n",
      "i: 300000\n",
      "i: 400000\n",
      "i: 500000\n",
      "config.device: cuda:0\n",
      "学習セット数: 12687\n",
      "評価セット数: 1586\n",
      "テストセット数: 50744\n",
      "use_amp: True\n",
      "use_saved_pth: False\n",
      "img_length: 577\n",
      "text_length_max: 97\n",
      "stride: 6\n",
      "bert in memory size: torch.Size([1, 97, 1024])\n",
      "initialize self.toplayer\n",
      "in TopLyaer:\n",
      "num_global_steps: 126870\n",
      "num_warmup_steps: 12687.0\n",
      "use_saved_pth: False\n",
      "exist saved_pth: False\n",
      "begin_epoch: 0\n",
      "global_ste: 0\n",
      "train_param: 2114\n",
      "val_param: 528\n",
      "epochs: 10\n",
      "batch_size: 32\n",
      "lr_clip: 2e-07\n",
      "lr_bert: 2e-05\n",
      "lr_others: 4e-05\n",
      "weight_decay: 0.1\n",
      "betas: (0.9, 0.999)\n",
      "train_loss_file: ./model/MyOriginal_train_loss_20251112_013826.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96024bdad11e4837819b0f3fba92f1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12687 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 1.5764168046031368e-11\n",
      "Train epoch = 0, loss = 2324.23681640625, WER = 248.5, BLEU = 13.77786636352539\n",
      "refe: on the left side, there is a person holding a glass and standing. on the right side, there is a person in white color shirt, wearing a cap, smiling and standing. and the background is blurred.\n",
      "hypo: tidessters prosecutorsters न grassland opportunity pretended grasslandtok 09 hurricanelston न scenario tak healy mug prosecutor orioles prosecutor am tak luggagesters pretended prosecutor prosecutor prosecutor mug prosecutor scenario prosecutor bai opportunity unnecessary prosecutorstersdilly scenariostersdilly inspire prosecutor prosecutor luggage luggagelston pretended scenario scenario prosecutor scenario scenariostersstersstersdilly consisting न prosecutor academies hurricane न prosecutor prosecutor न न न class prosecutor prosecutordilly न scenario न motive scenario न inspire prosecutor prosecutor scenariolston opportunity न न न groverdillysters scenariolston prosecutor inspiresterssters\n",
      "lr: 3.334121541735635e-08\n",
      "Train epoch = 0, loss = 645.5697021484375, WER = 89.48546600341797, BLEU = 5.120990753173828\n",
      "refe: in this picture we can see a glass with water in it, chair and some objects and in the background it is blurry.\n",
      "hypo: in is in in is a see is and in and is in a and in is and is\n",
      "lr: 6.666666666666665e-08\n",
      "Train epoch = 0, loss = 581.0928955078125, WER = 83.94224548339844, BLEU = 22.221216201782227\n",
      "refe: in this image i can see a person riding on car and person wearing a helmet.\n",
      "hypo: in this picture we can see a is is and is and is and and is and and and and and and and\n",
      "lr: 9.999211791597698e-08\n",
      "Train epoch = 0, loss = 520.9901123046875, WER = 83.01187133789062, BLEU = 22.3139705657959\n",
      "refe: this is a picture taken in a hall, there are two women's standing on the stage and holding the microphones. background of this image is a screen and on top of them there are the lights and smoke.\n",
      "hypo: in the picture there can see a man is standing and and and and and and and and and and and and and and and is and and and and\n",
      "lr: 1.3331756916528728e-07\n",
      "Train epoch = 0, loss = 470.5970458984375, WER = 82.48648071289062, BLEU = 21.668712615966797\n",
      "refe: in the picture we can see a wooden plank on it we can see a tortoise.\n",
      "hypo: in this picture i can see a is a a a...\n",
      "lr: 1.6664302041459763e-07\n",
      "Train epoch = 0, loss = 404.2894287109375, WER = 82.54020690917969, BLEU = 20.685108184814453\n",
      "refe: in this picture we can see clock on the wall, and also we can see a tree.\n",
      "hypo: in this image i can see building building a a the. the. the the the the.. the\n",
      "lr: 1.9996847166390794e-07\n",
      "Train epoch = 0, loss = 376.2780456542969, WER = 82.24307250976562, BLEU = 20.314319610595703\n",
      "refe: in the image there is baby standing by holding the paddle of a toy vehicle, she is wearing a magic hat and behind the baby there are few houses and on the left side there is a small garden beside the house. there is a car parked in front of one of the house on the right side, the climate is sunny.\n",
      "hypo: in this image i can a a a and a a and and and and. and the the. the and the\n",
      "Train epoch = 0, loss = 376.95550537109375, WER = 82.27011108398438, BLEU = 20.322092056274414\n",
      "refe: in the center of the image we can see one person with light brown color eyes. and we can see the green color cloth on him.\n",
      "hypo: in this image i can see a of a the....\n",
      "学習率: 2e-07\n",
      "Train loss: 376.95546295166014\n",
      "Train crf: 268.21739822387696\n",
      "Train ca: 108.73806549072266\n",
      "Train WER: 82.27010892337057\n",
      "Train BLEU: 20.32209308752713\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f315b0c2bf742ac942601db1e1bdc7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val epoch = 0, WER = 82.83582305908203, BLEU = 32.22361373901367\n",
      "refe: in this image i can see few trees which are green in color, few flowers which are red in color and in the background i can see a person standing, the road, few vehicles, few buildings, few trees and the sky.\n",
      "hypo: in this image we can see we can see windows some, there are, there are, plants tent street flowers some tent street flowers some, there are\n",
      "Val epoch = 0, WER = 83.60577392578125, BLEU = 35.748111724853516\n",
      "refe: in this picture, we see men in the uniform are standing. the man on the left side is holding a wooden stick. behind him, we see a man is holding a green color flag. we see people are holding flags which are in red, blue and green color. in the background, we see a car and buildings. there are poles and buildings in the background.\n",
      "hypo: in this image we can see a group of people on the wearing we can see on the wearing trees person few some tent street flowers some, there are\n",
      "Val epoch = 0, WER = 83.96411895751953, BLEU = 35.55498123168945\n",
      "refe: on the left side of the image we can see a person is standing near a bench. in the middle of the image we can see some clothes are hung to a hunger and two persons are there. on the right side of the image we can see a person and some clothes hung to a hunger.\n",
      "hypo: in this image we can see a girl view brown riding few who dress also to apron we can see on the wearing we can see on the background trees person few bottles\n",
      "Val epoch = 0, WER = 83.7027359008789, BLEU = 35.640254974365234\n",
      "refe: in the image we can see there is a painting on the wall of a dog standing and there is matter written on the wall. the wall is made up of stone bricks.\n",
      "hypo: in this image we can see graffiti a car on there is graffiti a graffiti a bottom having sheet\n",
      "Val epoch = 0, WER = 83.7190170288086, BLEU = 35.63145065307617\n",
      "refe: in front of the image there is a wooden log with a tool on it. beside that there is a machine. and also there are wooden poles with equipment. and there is a person holding the wooden object. and there are few people in the image. on the left corner of the image there is a table with fire. and in the background there are buildings, pole with a sign board, railings, walls, water and chain. on the ground there is a box,\n",
      "hypo: in this image we can see a people are them ladder men in the wearing we can see on the wearing we can see on the wearing we can see a stores i poles banners along racks clothes\n",
      "Validation WER: 83.71900853900148\n",
      "Validation BLEU: 35.63144950098201\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a167961c05457389a2c981c5b87e67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12687 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 1.9999824842577267e-07\n",
      "Train epoch = 1, loss = 355.1583251953125, WER = 84.11614227294922, BLEU = 20.631010055541992\n",
      "refe: in this picture we can see few houses, and few cars on the road, and also we can see a sign board, plants and cables.\n",
      "hypo: in this image i can see the building a a,,, and, and,, and, and, and and and and the and and and and\n",
      "lr: 1.9629542050918262e-07\n",
      "Train epoch = 1, loss = 322.8299255371094, WER = 82.82540893554688, BLEU = 19.79180145263672\n",
      "refe: in this image i can see on the left side a man is smiling, he wore t - shirt, spectacles. beside him a woman is also smiling, at the top it is the sky.\n",
      "hypo: in this image i the see two man and and and and and and and... and\n",
      "lr: 1.9259259259259257e-07\n",
      "Train epoch = 1, loss = 298.4232482910156, WER = 82.66929626464844, BLEU = 19.537235260009766\n",
      "refe: in this image, we can see shells and a tube on a cloth.\n",
      "hypo: in this image i can see a, on a and....\n",
      "lr: 1.8888976467600255e-07\n",
      "Train epoch = 1, loss = 291.91253662109375, WER = 83.52503967285156, BLEU = 19.64631462097168\n",
      "refe: in this image we can see a mountain covered with a few trees and plants, few rock stones and the sky.\n",
      "hypo: in this image i can see the,,, and and and and and and and\n",
      "lr: 1.851869367594125e-07\n",
      "Train epoch = 1, loss = 276.786376953125, WER = 84.10242462158203, BLEU = 20.036317825317383\n",
      "refe: in this image there is a chair on the floor. there is a cushion on the chair. beside there is a fan. right side there is a rack having monitor, books, cup and few objects. in the cup there is a scissor, pens and few objects. left side there is a toy on the floor. behind there is a table having a device and a frame on it. beside there is a rack having books, picture frames and few objects. there is\n",
      "hypo: in this image picture a a a,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "lr: 1.8148410884282249e-07\n",
      "Train epoch = 1, loss = 243.62863159179688, WER = 83.70988464355469, BLEU = 18.56874656677246\n",
      "refe: this picture shows people seated on the chairs and speaking to each other and we see few laptops papers and microphones on the table and we see couple of men standing\n",
      "hypo: in this image there are a people people are are are chairs and. and table table,,,,,,,,,,,,,,,,,,\n",
      "lr: 1.7778128092623244e-07\n",
      "Train epoch = 1, loss = 244.0689239501953, WER = 83.24642944335938, BLEU = 19.379276275634766\n",
      "refe: in this image i can see a blue colour thing and on it i can see few cartoons and i can also see depiction of few fishes, of a cat and here i can see something is written.\n",
      "hypo: in this image there can a a a of a, and and and and\n",
      "Train epoch = 1, loss = 244.79238891601562, WER = 83.26087188720703, BLEU = 19.270801544189453\n",
      "refe: on the table i can see the glass, plants, book, bowl and wine glass. in that bowl i can see the rice and other food item. beside the table i can see the chair which are placed near to the wall.\n",
      "hypo: in this image there is a table,, table,,,,,,,,,,,,,\n",
      "学習率: 1.7777777777777776e-07\n",
      "Train loss: 244.79237991333008\n",
      "Train crf: 144.29743759155272\n",
      "Train ca: 100.49494239807129\n",
      "Train WER: 83.26087218205834\n",
      "Train BLEU: 19.270801584318857\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3bbce6416049c9b3301e27113f8b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val epoch = 1, WER = 77.68656921386719, BLEU = 30.80805206298828\n",
      "refe: in this image i can see few trees which are green in color, few flowers which are red in color and in the background i can see a person standing, the road, few vehicles, few buildings, few trees and the sky.\n",
      "hypo: in this image i can see trees, plants, trees, there are trees, there are trees, we can see the sky.\n",
      "Val epoch = 1, WER = 76.66458892822266, BLEU = 35.931549072265625\n",
      "refe: in this picture, we see men in the uniform are standing. the man on the left side is holding a wooden stick. behind him, we see a man is holding a green color flag. we see people are holding flags which are in red, blue and green color. in the background, we see a car and buildings. there are poles and buildings in the background.\n",
      "hypo: in this image we can see a group of people wearing flags flags flags flags flags flags flags flags flags flags flags flags flags flags flags flags flags flags flags their hands.\n",
      "Val epoch = 1, WER = 77.03040313720703, BLEU = 35.522850036621094\n",
      "refe: on the left side of the image we can see a person is standing near a bench. in the middle of the image we can see some clothes are hung to a hunger and two persons are there. on the right side of the image we can see a person and some clothes hung to a hunger.\n",
      "hypo: in this image we can see a person is a standing, we can see here we can see here we can see here we can see here we can see the background there are trees.\n",
      "Val epoch = 1, WER = 76.86255645751953, BLEU = 35.44841384887695\n",
      "refe: in the image we can see there is a painting on the wall of a dog standing and there is matter written on the wall. the wall is made up of stone bricks.\n",
      "hypo: in this image we can see a coin can see the letters letters\n",
      "Val epoch = 1, WER = 76.89059448242188, BLEU = 35.43464660644531\n",
      "refe: in front of the image there is a wooden log with a tool on it. beside that there is a machine. and also there are wooden poles with equipment. and there is a person holding the wooden object. and there are few people in the image. on the left corner of the image there is a table with fire. and in the background there are buildings, pole with a sign board, railings, walls, water and chain. on the ground there is a box,\n",
      "hypo: in this image we can see a person is military as sheds we can see the right side, we can see a fence sheds we can see a fence sheds we can see the sky.\n",
      "Validation WER: 76.89058886375989\n",
      "Validation BLEU: 35.43464782291317\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acd671baf954387ab7dab76451671d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12687 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 1.7777602620355044e-07\n",
      "Train epoch = 2, loss = 258.4427185058594, WER = 82.35736083984375, BLEU = 22.3997859954834\n",
      "refe: as we can see in the image there is a wooden table. on table there is a paper and spectacles. on paper there is something written.\n",
      "hypo: in this image i a a a,, a and and and and a\n",
      "lr: 1.740731982869604e-07\n",
      "Train epoch = 2, loss = 230.9816436767578, WER = 83.2561264038086, BLEU = 18.892454147338867\n",
      "refe: in this image i can see a man holding a guitar and wearing a brown color shirt. on the left side a man stand and wearing a cap. on the left corner there is a text written on the image.\n",
      "hypo: in this image a guitar a a guitar a guitar guitar guitar a\n",
      "lr: 1.7037037037037037e-07\n",
      "Train epoch = 2, loss = 227.8808135986328, WER = 83.55486297607422, BLEU = 18.98621368408203\n",
      "refe: in this image there are few persons standing there are few bottles in fort of the persons on the table, at the side there is a popcorn cane, at the back ground there is a window and a wall.\n",
      "hypo: in this image there are see people standing standing standing and and and and and and and and and and and and and and and and\n",
      "lr: 1.6666754245378033e-07\n",
      "Train epoch = 2, loss = 244.0217742919922, WER = 83.11941528320312, BLEU = 19.072856903076172\n",
      "refe: in this image, there is a bag color red. this bag contains handles and belt.\n",
      "hypo: in this image a can a a bag bag bag\n",
      "lr: 1.629647145371903e-07\n",
      "Train epoch = 2, loss = 237.43629455566406, WER = 82.79559326171875, BLEU = 20.040325164794922\n",
      "refe: as we can see in the image there is a poster. on poster there is a chitha and sky.\n",
      "hypo: in this image poster a see poster of of a a and and and and and and and\n",
      "lr: 1.5926188662060026e-07\n",
      "Train epoch = 2, loss = 234.3041229248047, WER = 83.13702392578125, BLEU = 19.490816116333008\n",
      "refe: on the left there is a vehicle in white color, in the middle a man is standing, he wore a black color coat. beside him a woman is smiling, on the right side a person is shooting in the camera. there is a tree in this image and there are buildings at the backside of an image.\n",
      "hypo: in this image there can see there, and standing and and,,,,,,,,,,,,,,,,,,,,\n",
      "lr: 1.5555905870401024e-07\n",
      "Train epoch = 2, loss = 228.26113891601562, WER = 82.82113647460938, BLEU = 21.455703735351562\n",
      "refe: in this image we can see a building, sign board, ice sculptures, persons standing on the floor, serving plates, flower vases, polythene covers and decors.\n",
      "hypo: in this image i can see there, and standing and and and and and and and and and and and and and and and and and and and\n",
      "Train epoch = 2, loss = 229.51011657714844, WER = 82.8555679321289, BLEU = 21.314516067504883\n",
      "refe: in this image i can see three persons standing. there is a table and chair. the person is wearing a blue blazer and a white pant. at the back side i can see a water, tube and a stairs and on the left side i can see a tent and a trees. there is a building. the woman is wearing a white fork.\n",
      "hypo: in this image i can a two man and standing and and and and and and. and and and and and and and\n",
      "学習率: 1.5555555555555556e-07\n",
      "Train loss: 229.51010345458985\n",
      "Train crf: 130.02864097595216\n",
      "Train ca: 99.48146186828613\n",
      "Train WER: 82.85557038962042\n",
      "Train BLEU: 21.314518191603014\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4764bf5af24e0e83cdf3c2be6223b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val epoch = 2, WER = 76.11940002441406, BLEU = 34.012752532958984\n",
      "refe: in this image i can see few trees which are green in color, few flowers which are red in color and in the background i can see a person standing, the road, few vehicles, few buildings, few trees and the sky.\n",
      "hypo: in this image we can see few plants, plants, plants, plants, we can see the background we can see the sky.\n",
      "Val epoch = 2, WER = 76.2474365234375, BLEU = 36.7791633605957\n",
      "refe: in this picture, we see men in the uniform are standing. the man on the left side is holding a wooden stick. behind him, we see a man is holding a green color flag. we see people are holding flags which are in red, blue and green color. in the background, we see a car and buildings. there are poles and buildings in the background.\n",
      "hypo: in this image we can see a group of people standing and holding some their hands. in the their hands. in the background we can see the background we can see a flag.\n",
      "Val epoch = 2, WER = 76.4533920288086, BLEU = 36.41447830200195\n",
      "refe: on the left side of the image we can see a person is standing near a bench. in the middle of the image we can see some clothes are hung to a hunger and two persons are there. on the right side of the image we can see a person and some clothes hung to a hunger.\n",
      "hypo: in this image we can see a person standing on the ground. in front of the ground. in front of the background there is carrying a person is carrying a person is carrying a trolley.\n",
      "Val epoch = 2, WER = 76.32955169677734, BLEU = 36.4219856262207\n",
      "refe: in the image we can see there is a painting on the wall of a dog standing and there is matter written on the wall. the wall is made up of stone bricks.\n",
      "hypo: in this image we can see a poster on the wall, we can see some text.\n",
      "Val epoch = 2, WER = 76.32425689697266, BLEU = 36.397586822509766\n",
      "refe: in front of the image there is a wooden log with a tool on it. beside that there is a machine. and also there are wooden poles with equipment. and there is a person holding the wooden object. and there are few people in the image. on the left corner of the image there is a table with fire. and in the background there are buildings, pole with a sign board, railings, walls, water and chain. on the ground there is a box,\n",
      "hypo: in this image we can see a person standing on the image, there is a trolley. in front of the right side of the right side of the right side of the background we can see a trolley.\n",
      "Validation WER: 76.32425163693506\n",
      "Validation BLEU: 36.39758581748271\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9588c797c9ab4cd1a83e662e7b8133ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12687 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 1.555538039813282e-07\n",
      "Train epoch = 3, loss = 209.12078857421875, WER = 82.47248077392578, BLEU = 25.95050621032715\n",
      "refe: in this picture we can see a woman and she is holding a cat.\n",
      "hypo: in this image there can a a a and a a and and and\n",
      "lr: 1.5185097606473817e-07\n",
      "Train epoch = 3, loss = 224.85279846191406, WER = 82.86471557617188, BLEU = 21.534082412719727\n",
      "refe: in the foreground of this picture we can see a man seems to be standing. in the background we can see some other items.\n",
      "hypo: in this image there can a a person. the and and and and\n",
      "lr: 1.4814814814814815e-07\n",
      "Train epoch = 3, loss = 232.16876220703125, WER = 82.41110229492188, BLEU = 21.437036514282227\n",
      "refe: in this image there is a man standing on the ground. he is holding a golf stick in his hand. there is grass on the ground. in the background there are trees and a building. at the top there is the sky.\n",
      "hypo: in this image of the a a person a the the,,,,,, the the the the,, the,,,,,,\n",
      "lr: 1.444453202315581e-07\n",
      "Train epoch = 3, loss = 215.69076538085938, WER = 82.27233123779297, BLEU = 21.651033401489258\n",
      "refe: this looks like a skyscraper. i can see the street light. this looks like a tree with leaves. here is the sky, which is blue in color.\n",
      "hypo: in this image i can see building building, and and and the the the the the\n",
      "lr: 1.4074249231496805e-07\n",
      "Train epoch = 3, loss = 211.27352905273438, WER = 82.32901000976562, BLEU = 22.2037296295166\n",
      "refe: in the image there is a man holding a microphone and he is also wearing a black color suit, in background we can see hoardings.\n",
      "hypo: in this image i can see a man holding the holding and and and and\n",
      "lr: 1.3703966439837803e-07\n",
      "Train epoch = 3, loss = 209.8666534423828, WER = 82.3186264038086, BLEU = 21.931079864501953\n",
      "refe: in this picture we can see a woman standing on the ground, she is holding a broomstick, here we can see plants, grass, flip flops, steps and some objects and in the background we can see a wall, curtains.\n",
      "hypo: in this image there the a a a a the the and the and the the the the the the the the\n",
      "lr: 1.3333683648178798e-07\n",
      "Train epoch = 3, loss = 209.12144470214844, WER = 82.14871978759766, BLEU = 21.726228713989258\n",
      "refe: this is a poster and in this poster we can see a doll on the surface, some text and in the background it is dark.\n",
      "hypo: in this image i can see a doll a a a a a a a a\n",
      "Train epoch = 3, loss = 208.41526794433594, WER = 82.09516906738281, BLEU = 21.782075881958008\n",
      "refe: in the image we can see grass, water, plants, hills and the sky.\n",
      "hypo: in this image there can see water,,,, the the the the the the the the\n",
      "学習率: 1.333333333333333e-07\n",
      "Train loss: 208.41527130126954\n",
      "Train crf: 111.6669596862793\n",
      "Train ca: 96.74831150054932\n",
      "Train WER: 82.09517055990682\n",
      "Train BLEU: 21.782075173060626\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "595928443a3a4abaaadb1a7b8e09437f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val epoch = 3, WER = 77.6119384765625, BLEU = 27.903629302978516\n",
      "refe: in this image i can see few trees which are green in color, few flowers which are red in color and in the background i can see a person standing, the road, few vehicles, few buildings, few trees and the sky.\n",
      "hypo: in this image we can see plants, plants, plants, we can see the background there are trees.\n",
      "Val epoch = 3, WER = 76.19596862792969, BLEU = 31.117887496948242\n",
      "refe: in this picture, we see men in the uniform are standing. the man on the left side is holding a wooden stick. behind him, we see a man is holding a green color flag. we see people are holding flags which are in red, blue and green color. in the background, we see a car and buildings. there are poles and buildings in the background.\n",
      "hypo: in this image we can see a group of people standing and holding a flag army their hands. in the background we can see the background there are trees.\n",
      "Val epoch = 3, WER = 76.4884033203125, BLEU = 30.741113662719727\n",
      "refe: on the left side of the image we can see a person is standing near a bench. in the middle of the image we can see some clothes are hung to a hunger and two persons are there. on the right side of the image we can see a person and some clothes hung to a hunger.\n",
      "hypo: in this image we can see a man standing on the image there is a dustbin, we can see a dustbin, we can see some other objects.\n",
      "Val epoch = 3, WER = 76.24444580078125, BLEU = 30.754274368286133\n",
      "refe: in the image we can see there is a painting on the wall of a dog standing and there is matter written on the wall. the wall is made up of stone bricks.\n",
      "hypo: in this image we can see a poster on the wall.\n",
      "Val epoch = 3, WER = 76.27368927001953, BLEU = 30.730257034301758\n",
      "refe: in front of the image there is a wooden log with a tool on it. beside that there is a machine. and also there are wooden poles with equipment. and there is a person holding the wooden object. and there are few people in the image. on the left corner of the image there is a table with fire. and in the background there are buildings, pole with a sign board, railings, walls, water and chain. on the ground there is a box,\n",
      "hypo: in this image we can see a few people standing on the right side of the right side of the right side of the right side there is carrying a trolley.\n",
      "Validation WER: 76.27368555509186\n",
      "Validation BLEU: 30.730257939772482\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da55380b43e049f98b280a2927a169dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12687 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 1.3333158175910599e-07\n",
      "Train epoch = 4, loss = 177.74798583984375, WER = 77.93696594238281, BLEU = 25.757362365722656\n",
      "refe: in this image we can see a boat in which a person is sitting is floating on the water. here we can see this person is standing on the rock. in the background, we can see trees and sky with the clouds.\n",
      "hypo: in this image there can see there, on water water the the the the the the the the the the the the\n",
      "lr: 1.2962875384251594e-07\n",
      "Train epoch = 4, loss = 201.44859313964844, WER = 82.182861328125, BLEU = 21.54418182373047\n",
      "refe: here we can see a toy and puzzles. there is a carpet on the floor. in the background we can see a wall.\n",
      "hypo: in this image of can see a a a a the, the the the, the the the the the the the\n",
      "lr: 1.2592592592592592e-07\n",
      "Train epoch = 4, loss = 195.2248992919922, WER = 82.06240844726562, BLEU = 23.070087432861328\n",
      "refe: in this picture we can see a person is holding a dog belt and walking and the dog is also walking on the path.\n",
      "hypo: in this image i can a a a a the a a the the the the the\n",
      "lr: 1.2222309800933587e-07\n",
      "Train epoch = 4, loss = 198.57980346679688, WER = 82.211181640625, BLEU = 23.117582321166992\n",
      "refe: in the image i can see two people who has some posters to the dresses and also i can see some people, building and a screen on which some text is displayed.\n",
      "hypo: in this image there the see there a and the and and the the the and the the the,,,,,,,,,,,,\n",
      "lr: 1.1852027009274585e-07\n",
      "Train epoch = 4, loss = 211.58609008789062, WER = 81.85092163085938, BLEU = 21.791183471679688\n",
      "refe: in this picture we can see chairs with pillows on it, tables on the floor, books, remote, television, devices, house plants, candle stands, some objects and in the background we can see the wall.\n",
      "hypo: in this image we in a a,,,,,,,,,,,,,,,,,\n",
      "lr: 1.148174421761558e-07\n",
      "Train epoch = 4, loss = 210.28311157226562, WER = 82.16078186035156, BLEU = 21.777667999267578\n",
      "refe: in this image, we can see a bride and groom in a white dress. on the right side, we can see a flower bouquet and two women. background there is a wall. here we can see a woman is holding a mobile and the other woman is wearing a hat.\n",
      "hypo: in this image there can see there and and a and and and and........\n",
      "lr: 1.1111461425956578e-07\n",
      "Train epoch = 4, loss = 208.38644409179688, WER = 81.97541046142578, BLEU = 22.6416015625\n",
      "refe: in the foreground of the picture there are people on the street. in the background there are trees, buildings, signal lights and other objects.\n",
      "hypo: in this image there can see people people people people and and the and and the the the.....\n",
      "Train epoch = 4, loss = 209.73370361328125, WER = 81.9892349243164, BLEU = 22.482166290283203\n",
      "refe: in this image at the bottom there is a river and some rocks, in the background there are a group of trees.\n",
      "hypo: in this image there can see the,, rocks and and rocks the the the\n",
      "学習率: 1.1111111111111111e-07\n",
      "Train loss: 209.73371780395507\n",
      "Train crf: 112.33239402770997\n",
      "Train ca: 97.40132431030274\n",
      "Train WER: 81.9892359462861\n",
      "Train BLEU: 22.482164042103253\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d7487c5cbb4e8295fb5ff34f937a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val epoch = 4, WER = 74.25373077392578, BLEU = 31.49457359313965\n",
      "refe: in this image i can see few trees which are green in color, few flowers which are red in color and in the background i can see a person standing, the road, few vehicles, few buildings, few trees and the sky.\n",
      "hypo: in this image we can see a group of the foreground of the background we can see the background we can see the sky.\n",
      "Val epoch = 4, WER = 75.41302490234375, BLEU = 34.570377349853516\n",
      "refe: in this picture, we see men in the uniform are standing. the man on the left side is holding a wooden stick. behind him, we see a man is holding a green color flag. we see people are holding flags which are in red, blue and green color. in the background, we see a car and buildings. there are poles and buildings in the background.\n",
      "hypo: in this image we can see a group of people standing and holding a flag. in their hands. in the background we can see the background we can see a flag.\n",
      "Val epoch = 4, WER = 75.88600158691406, BLEU = 34.03779220581055\n",
      "refe: on the left side of the image we can see a person is standing near a bench. in the middle of the image we can see some clothes are hung to a hunger and two persons are there. on the right side of the image we can see a person and some clothes hung to a hunger.\n",
      "hypo: in this image we can see a man standing on the ground, trouser, trouser, we can see the left side of the background we can see the background we can see the road.\n",
      "Val epoch = 4, WER = 75.58411407470703, BLEU = 34.036624908447266\n",
      "refe: in the image we can see there is a painting on the wall of a dog standing and there is matter written on the wall. the wall is made up of stone bricks.\n",
      "hypo: in this image we can see a poster, we can see a wall.\n",
      "Val epoch = 4, WER = 75.64132690429688, BLEU = 33.974281311035156\n",
      "refe: in front of the image there is a wooden log with a tool on it. beside that there is a machine. and also there are wooden poles with equipment. and there is a person holding the wooden object. and there are few people in the image. on the left corner of the image there is a table with fire. and in the background there are buildings, pole with a sign board, railings, walls, water and chain. on the ground there is a box,\n",
      "hypo: in this image we can see a man standing on the right side of the right side of the right side of the right side of the background we can see the background we can see the barrels\n",
      "Validation WER: 75.64133336695944\n",
      "Validation BLEU: 33.974283294581916\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7331de53235e467ab5ef0211f1bf838e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12687 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 1.1110935953688376e-07\n",
      "Train epoch = 5, loss = 197.68997192382812, WER = 82.30277252197266, BLEU = 18.53028106689453\n",
      "refe: in the middle of the picture, we see a toilet seat. on the right side, we see the tissue rolls and a sanitizer. behind that, we see a wall in brown color. in the background, we see a brown wall. this picture might be clicked in the washroom.\n",
      "hypo: in this image i can a a a, a,,,,,,,,,,,\n",
      "lr: 1.0740653162029374e-07\n",
      "Train epoch = 5, loss = 203.94483947753906, WER = 82.1174545288086, BLEU = 22.897539138793945\n",
      "refe: in this image, there is a person wearing clothes and cap. this person is standing in front of this mic and playing a guitar.\n",
      "hypo: in this image a in a a a a a a a a and and\n",
      "lr: 1.0370370370370369e-07\n",
      "Train epoch = 5, loss = 209.6751708984375, WER = 81.61355590820312, BLEU = 22.411489486694336\n",
      "refe: here in this picture we can see rock stones present in the middle of the river over there and we can see people travelling in the water with the help of lifeboats present with them over there.\n",
      "hypo: in this image there can see there in in the in the the the the the the the\n",
      "lr: 1.0000087578711367e-07\n",
      "Train epoch = 5, loss = 206.4049072265625, WER = 81.92767333984375, BLEU = 22.237136840820312\n",
      "refe: in this image there are plants towards the right of the image, there are plants towards the left of the image, there is soil on the ground, there is a roof towards the top of the image.\n",
      "hypo: in this image there can see there,, plants the, the the the the the the\n",
      "lr: 9.629804787052362e-08\n",
      "Train epoch = 5, loss = 191.48617553710938, WER = 81.7193374633789, BLEU = 23.732275009155273\n",
      "refe: in this picture, we see balls in purple, red, blue, green and yellow color. beside that, we see a net. at the bottom of the picture, we see a carpet or a floor in grey and brown color. in the background, we see a pole. in the background, it is black in color. this picture might be clicked in the indoor stadium.\n",
      "hypo: in this image there can see see balls balls balls balls balls the balls balls balls balls\n",
      "lr: 9.259521995393359e-08\n",
      "Train epoch = 5, loss = 196.5874786376953, WER = 81.94940185546875, BLEU = 22.11783218383789\n",
      "refe: in this image i can see few plants which are green in color and few flowers which are orange and yellow in color. i can see a statue which is green and black in color. in the background i can see few vehicles on the ground, few trees, a building, few antennas on the building, the sky and few poles.\n",
      "hypo: in this image of can see the a, the,,, the the the,, the,,,,,,,,,,,,,,\n",
      "lr: 8.889239203734356e-08\n",
      "Train epoch = 5, loss = 203.46324157714844, WER = 82.04976654052734, BLEU = 21.87109375\n",
      "refe: in this picture we can see a group of people standing on the floor, guitar, caps, cameras, posters, pillars, lights and some objects and in the background we can see the wall.\n",
      "hypo: in this image i can see a people people people are. the the..\n",
      "Train epoch = 5, loss = 203.08238220214844, WER = 82.06285095214844, BLEU = 21.828866958618164\n",
      "refe: in this image we can see the buildings, in front of the buildings, we can see there are trees, railing, street light, rods and few objects on the ground. we can see the wall and sky in the background.\n",
      "hypo: in this image there can see buildings,,, and, and and\n",
      "学習率: 8.888888888888888e-08\n",
      "Train loss: 203.08237640380858\n",
      "Train crf: 105.29152641296386\n",
      "Train ca: 97.79084991455078\n",
      "Train WER: 82.0628463132062\n",
      "Train BLEU: 21.82886696049738\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c7d1c6aeea4d0fb2ba44ef7a6eae73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val epoch = 5, WER = 75.74626922607422, BLEU = 29.367403030395508\n",
      "refe: in this image i can see few trees which are green in color, few flowers which are red in color and in the background i can see a person standing, the road, few vehicles, few buildings, few trees and the sky.\n",
      "hypo: in this image we can see plants, plants, plants, trees, there are trees.\n",
      "Val epoch = 5, WER = 75.7435302734375, BLEU = 32.318565368652344\n",
      "refe: in this picture, we see men in the uniform are standing. the man on the left side is holding a wooden stick. behind him, we see a man is holding a green color flag. we see people are holding flags which are in red, blue and green color. in the background, we see a car and buildings. there are poles and buildings in the background.\n",
      "hypo: in this image we can see a group of people holding a flag. in their hands. in their hands. in the background we can see a flag.\n",
      "Val epoch = 5, WER = 76.16816711425781, BLEU = 31.8616943359375\n",
      "refe: on the left side of the image we can see a person is standing near a bench. in the middle of the image we can see some clothes are hung to a hunger and two persons are there. on the right side of the image we can see a person and some clothes hung to a hunger.\n",
      "hypo: in this image we can see a person standing on the image, trouser, trouser, we can see the background we can see a tree.\n",
      "Val epoch = 5, WER = 75.9614028930664, BLEU = 32.12417984008789\n",
      "refe: in the image we can see there is a painting on the wall of a dog standing and there is matter written on the wall. the wall is made up of stone bricks.\n",
      "hypo: in this image we can see a poster on the wall. on the wall.\n",
      "Val epoch = 5, WER = 76.00102233886719, BLEU = 32.0748405456543\n",
      "refe: in front of the image there is a wooden log with a tool on it. beside that there is a machine. and also there are wooden poles with equipment. and there is a person holding the wooden object. and there are few people in the image. on the left corner of the image there is a table with fire. and in the background there are buildings, pole with a sign board, railings, walls, water and chain. on the ground there is a box,\n",
      "hypo: in this image we can see a person standing on the right side of the right side of the right side of the right side of the background we can see some objects.\n",
      "Validation WER: 76.00102814281082\n",
      "Validation BLEU: 32.074836750213706\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eecc0a96a2fb4c678bc18dc6e5f4b554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12687 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 8.888713731466156e-08\n",
      "Train epoch = 6, loss = 183.91311645507812, WER = 83.28267669677734, BLEU = 18.535940170288086\n",
      "refe: in this picture we can see three persons are sitting in front of them there is a table and one person is talking and the rest of two persons are looking towards the opposite back side we can see a black color wall\n",
      "hypo: in this image i sitting see three chairs on on chairs chairs chairs. the\n",
      "lr: 8.518430939807151e-08\n",
      "Train epoch = 6, loss = 197.7929229736328, WER = 81.56292724609375, BLEU = 23.555686950683594\n",
      "refe: in the middle of the image we can see a paper, on the paper there is drawing. behind the paper there is wall.\n",
      "hypo: in this image there can see a on on on on on on on and\n",
      "lr: 8.148148148148148e-08\n",
      "Train epoch = 6, loss = 184.09291076660156, WER = 81.39913177490234, BLEU = 23.057422637939453\n",
      "refe: in this image in the center there are plants. on the right side there is a door and a red colour stand and there is a wall. on the right side of the wall there is a frame. in front of the wall there are plants. on the left side there is a staircase.\n",
      "hypo: in this image i can see the,,,,,,,,,,,,,,, the\n",
      "lr: 7.777865356489144e-08\n",
      "Train epoch = 6, loss = 196.06739807128906, WER = 81.65184020996094, BLEU = 23.01811981201172\n",
      "refe: in this picture there are people, among them there is a person carrying a bag and we can see frame on the wall. at the top of the image we can see lights.\n",
      "hypo: in this image i can see a a people standing standing the the the the holding the.. the. the. the.\n",
      "lr: 7.407582564830141e-08\n",
      "Train epoch = 6, loss = 188.12937927246094, WER = 81.47409057617188, BLEU = 21.842077255249023\n",
      "refe: this image consists of a man wearing a gray t - shirt. a rope is tied to his hand. at the bottom, there is green grass on ground. in the background, there are many trees. at the top, there is sky.\n",
      "hypo: in this image there can see a a a standing the the the the the the the the the the the the the\n",
      "lr: 7.037299773171136e-08\n",
      "Train epoch = 6, loss = 188.12164306640625, WER = 81.46601867675781, BLEU = 21.89568519592285\n",
      "refe: in the foreground of the image we can see the grass. in the middle of the image we can see the building, two persons, some vehicles, road and water body. on the top of the image we can see the sky.\n",
      "hypo: in this image there can see there, the,,,,,,,,,,,,,,,, the, the the\n",
      "lr: 6.667016981512133e-08\n",
      "Train epoch = 6, loss = 191.1425323486328, WER = 81.55066680908203, BLEU = 21.889982223510742\n",
      "refe: in the image in the center, we can see trees, benches, plants, grass and dry leaves.\n",
      "hypo: in this image we can see the trees the leaves leaves leaves and leaves\n",
      "Train epoch = 6, loss = 192.02232360839844, WER = 81.53223419189453, BLEU = 21.744901657104492\n",
      "refe: in this image sky truncated towards the top of the image, there are buildings, there is a building truncated towards the right of the image, there is a building truncated towards the left of the image, there are trees, there is a tree truncated towards the top of the image, there is a tree truncated towards the left of the image, there is a tree truncated towards the right of the image, there is a fencing truncated, there are plants, there is grass truncated towards the\n",
      "hypo: in this image there can see there,,,,,,,,,,,, the the the\n",
      "学習率: 6.666666666666665e-08\n",
      "Train loss: 192.02233413696288\n",
      "Train crf: 94.58507083892822\n",
      "Train ca: 97.4372632598877\n",
      "Train WER: 81.53223242746805\n",
      "Train BLEU: 21.744903936228187\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4102ea20e4614d94ab65bbffc582d005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val epoch = 6, WER = 74.62686920166016, BLEU = 29.446977615356445\n",
      "refe: in this image i can see few trees which are green in color, few flowers which are red in color and in the background i can see a person standing, the road, few vehicles, few buildings, few trees and the sky.\n",
      "hypo: in this image we can see plants, plants, plants. in the background we can see the sky.\n",
      "Val epoch = 6, WER = 75.63579559326172, BLEU = 31.968101501464844\n",
      "refe: in this picture, we see men in the uniform are standing. the man on the left side is holding a wooden stick. behind him, we see a man is holding a green color flag. we see people are holding flags which are in red, blue and green color. in the background, we see a car and buildings. there are poles and buildings in the background.\n",
      "hypo: in this image we can see a group of people standing and holding a flag. in their hands. in the background we can see a flag.\n",
      "Val epoch = 6, WER = 76.09152221679688, BLEU = 31.39946937561035\n",
      "refe: on the left side of the image we can see a person is standing near a bench. in the middle of the image we can see some clothes are hung to a hunger and two persons are there. on the right side of the image we can see a person and some clothes hung to a hunger.\n",
      "hypo: in this image we can see a person wearing black color t - shirt, trouser, we can see the right side of the background we can see a tree.\n",
      "Val epoch = 6, WER = 75.89311218261719, BLEU = 31.507614135742188\n",
      "refe: in the image we can see there is a painting on the wall of a dog standing and there is matter written on the wall. the wall is made up of stone bricks.\n",
      "hypo: in this image we can see a poster on the wall.\n",
      "Val epoch = 6, WER = 75.91181945800781, BLEU = 31.516469955444336\n",
      "refe: in front of the image there is a wooden log with a tool on it. beside that there is a machine. and also there are wooden poles with equipment. and there is a person holding the wooden object. and there are few people in the image. on the left corner of the image there is a table with fire. and in the background there are buildings, pole with a sign board, railings, walls, water and chain. on the ground there is a box,\n",
      "hypo: in this image we can see a person standing on the image there is a person standing on the right side of the right side of the background we can see some objects.\n",
      "Validation WER: 75.9118162130405\n",
      "Validation BLEU: 31.516471945301813\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96123e64e5e54f12bc401304622dc203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12687 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 6.666491509243932e-08\n",
      "Train epoch = 7, loss = 198.85076904296875, WER = 79.14764404296875, BLEU = 21.86953353881836\n",
      "refe: in this picture, we see a man in the white t - shirt is running. at the bottom, we see the pavement or the soil. in the left top, we see the grass. on the right side, we see a man in the black t - shirt is standing. in the right top, we see the grass. this is a black and white picture. this picture might be clicked in the playground.\n",
      "hypo: in this image black and white image. in the the ground the the the the the the the.\n",
      "lr: 6.296208717584928e-08\n",
      "Train epoch = 7, loss = 196.3692626953125, WER = 81.69368743896484, BLEU = 22.000137329101562\n",
      "refe: in this image i can see two women are standing in the front and i can also see smile on their faces. i can see both of them are wearing jackets and i can see the left one is holding few things. i can also see the right one is wearing a cap. in the background i can see an open grass ground, number of trees, few buildings, a car and the sky. i can also see an iron gate on the left side of the image and on\n",
      "hypo: in this image there can see two women. one,, the the. the the the the the,,, the\n",
      "lr: 5.925925925925925e-08\n",
      "Train epoch = 7, loss = 183.06362915039062, WER = 81.2160873413086, BLEU = 22.7233829498291\n",
      "refe: here we can see stones on the ground. in the background there are trees, mountains and clouds in the sky.\n",
      "hypo: in this image i can see the the,, and and the the the the the\n",
      "lr: 5.555643134266922e-08\n",
      "Train epoch = 7, loss = 188.20574951171875, WER = 81.38960266113281, BLEU = 22.42205238342285\n",
      "refe: in this picture there is a pink color flower and there are buds on the plant. at the back the image is blurry.\n",
      "hypo: in this image i can see a flower flower flower and and and.\n",
      "lr: 5.185360342607918e-08\n",
      "Train epoch = 7, loss = 178.11871337890625, WER = 81.4217758178711, BLEU = 22.801244735717773\n",
      "refe: in this image i can see the dark picture in which i can see few bottles, few glasses, few clothes, a plastic box, a dog and few persons. in the background i can see the ceiling, few cupboards, the window, a flag and a cream colored object to the ceiling.\n",
      "hypo: in this image i can see there people,,,, and the,,,,,.,\n",
      "lr: 4.815077550948915e-08\n",
      "Train epoch = 7, loss = 177.65003967285156, WER = 81.2015151977539, BLEU = 22.357868194580078\n",
      "refe: in this image i can see a bed which is in cream color. background i can see wall in cream and white color.\n",
      "hypo: in this image i can see a a on on on, and the the the\n",
      "lr: 4.4447947592899116e-08\n",
      "Train epoch = 7, loss = 184.1251983642578, WER = 81.06255340576172, BLEU = 23.222679138183594\n",
      "refe: in this picture we can see there are some people are sitting on chairs and some people are standing and a person is holding a camera. in front of the people there are tables, covered with clothes and on the tables there are flower vases, glasses and other things. behind the people there is a blurred background.\n",
      "hypo: in this image we can see a sitting sitting sitting chairs and chairs. and and and and and\n",
      "Train epoch = 7, loss = 183.3343963623047, WER = 81.12389373779297, BLEU = 23.176191329956055\n",
      "refe: this is a black and white image, where we can see windows, wall, tree, creeper and the dried leaves on the ground.\n",
      "hypo: in this image black and white image. and this a we the the the the the and the\n",
      "学習率: 4.444444444444444e-08\n",
      "Train loss: 183.33438926696778\n",
      "Train crf: 87.97258792877197\n",
      "Train ca: 95.36180145263671\n",
      "Train WER: 81.12389189770278\n",
      "Train BLEU: 23.176191617100784\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760d3ff2fd114189b075f22d2194f1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val epoch = 7, WER = 76.2686538696289, BLEU = 27.913833618164062\n",
      "refe: in this image i can see few trees which are green in color, few flowers which are red in color and in the background i can see a person standing, the road, few vehicles, few buildings, few trees and the sky.\n",
      "hypo: in this image we can see some plants, plants, plants. in the background we can see the sky.\n",
      "Val epoch = 7, WER = 76.17823791503906, BLEU = 30.900772094726562\n",
      "refe: in this picture, we see men in the uniform are standing. the man on the left side is holding a wooden stick. behind him, we see a man is holding a green color flag. we see people are holding flags which are in red, blue and green color. in the background, we see a car and buildings. there are poles and buildings in the background.\n",
      "hypo: in this image we can see a group of people standing on the ground. in their hands. in the background there is a flag.\n",
      "Val epoch = 7, WER = 76.39102172851562, BLEU = 30.62799644470215\n",
      "refe: on the left side of the image we can see a person is standing near a bench. in the middle of the image we can see some clothes are hung to a hunger and two persons are there. on the right side of the image we can see a person and some clothes hung to a hunger.\n",
      "hypo: in this image we can see a person standing on the image there are standing on the right side of the right side of the right side of the background there are trees.\n",
      "Val epoch = 7, WER = 76.29100799560547, BLEU = 30.636058807373047\n",
      "refe: in the image we can see there is a painting on the wall of a dog standing and there is matter written on the wall. the wall is made up of stone bricks.\n",
      "hypo: in this image we can see a poster on the wall. on the wall.\n",
      "Val epoch = 7, WER = 76.31546783447266, BLEU = 30.597736358642578\n",
      "refe: in front of the image there is a wooden log with a tool on it. beside that there is a machine. and also there are wooden poles with equipment. and there is a person holding the wooden object. and there are few people in the image. on the left corner of the image there is a table with fire. and in the background there are buildings, pole with a sign board, railings, walls, water and chain. on the ground there is a box,\n",
      "hypo: in this image we can see a person standing on the image there is a wooden fence, trouser, we can see the background we can see some objects.\n",
      "Validation WER: 76.31546618467063\n",
      "Validation BLEU: 30.597734805491537\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f0d00397314bcb96b421a9ca622b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12687 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 4.444269287021711e-08\n",
      "Train epoch = 8, loss = 170.0294952392578, WER = 82.37671661376953, BLEU = 22.276729583740234\n",
      "refe: it looks like a black and white picture. we can see a woman and behind the woman there is a dark background.\n",
      "hypo: in this image black and white image.. woman a a a.\n",
      "lr: 4.073986495362707e-08\n",
      "Train epoch = 8, loss = 184.02078247070312, WER = 81.2964859008789, BLEU = 22.610116958618164\n",
      "refe: in this image we can see a group of plants. we can also see some poles, a fence, a pathway and some tents. on the backside we can see a group of trees, the mountains and the sky which looks cloudy.\n",
      "hypo: in this image there can see the,,,,,,,, the, the, the the\n",
      "lr: 3.7037037037037036e-08\n",
      "Train epoch = 8, loss = 174.8050994873047, WER = 81.10934448242188, BLEU = 22.545743942260742\n",
      "refe: in this image, there are some chairs which are in black color and there are some tables which are covered by a blue color cloth, there is a black color wall in the middle, there are some people sitting on the chairs around the tables, in the right side there are some white color glass window and there are some green color plants.\n",
      "hypo: in this image i can see the tables chairs tables chairs chairs chairs tables and and and..... and. and... and. and.\n",
      "lr: 3.3334209120446996e-08\n",
      "Train epoch = 8, loss = 179.43113708496094, WER = 81.39358520507812, BLEU = 22.577747344970703\n",
      "refe: in the center of the image we can see cars, petrol bunk, boards are there. at the bottom of the image a road is there. in the background of the image we can see a trees, shed, lights are present.\n",
      "hypo: in this image i can see there,, the,, the the, the, the the the the the the the the\n",
      "lr: 2.9631381203856963e-08\n",
      "Train epoch = 8, loss = 175.93336486816406, WER = 81.30469512939453, BLEU = 23.364910125732422\n",
      "refe: in this image, we can see food on the plate and at the bottom, there is a table.\n",
      "hypo: in this image i can see a food in a in the the\n",
      "lr: 2.592855328726693e-08\n",
      "Train epoch = 8, loss = 178.78253173828125, WER = 81.21220397949219, BLEU = 23.803464889526367\n",
      "refe: a water bottle and camera on a table which is beside a house plant.\n",
      "hypo: in this image a bottle a bottle bottle bottle bottle bottle a a a a a\n",
      "lr: 2.2225725370676896e-08\n",
      "Train epoch = 8, loss = 180.60882568359375, WER = 81.2256851196289, BLEU = 22.06479263305664\n",
      "refe: in this picture we can see two people sitting on the chair in front of a table and on table we have some glasses, bottles and some food. behind them there is a window which has a curtain and we can see cars and some trees.\n",
      "hypo: in this image i sitting see a. a sitting on a table table table table table table table table table table table table, and and\n",
      "Train epoch = 8, loss = 180.45394897460938, WER = 81.182861328125, BLEU = 22.124523162841797\n",
      "refe: in this image i can see a building, in front of the building i can see a staircase and green color fence and trees and at the top i can see the sky.\n",
      "hypo: in this image there can see there,,,,,,,,,,,, the the the\n",
      "学習率: 2.222222222222222e-08\n",
      "Train loss: 180.45394912719726\n",
      "Train crf: 85.49421272277831\n",
      "Train ca: 94.95973678588867\n",
      "Train WER: 81.18286124170316\n",
      "Train BLEU: 22.124523432352888\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49913a4b920e4cd7b7943e43d2d7d92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val epoch = 8, WER = 76.3432846069336, BLEU = 29.510948181152344\n",
      "refe: in this image i can see few trees which are green in color, few flowers which are red in color and in the background i can see a person standing, the road, few vehicles, few buildings, few trees and the sky.\n",
      "hypo: in this image we can see plants, plants, plants, plants, there are trees.\n",
      "Val epoch = 8, WER = 76.07099914550781, BLEU = 31.37432289123535\n",
      "refe: in this picture, we see men in the uniform are standing. the man on the left side is holding a wooden stick. behind him, we see a man is holding a green color flag. we see people are holding flags which are in red, blue and green color. in the background, we see a car and buildings. there are poles and buildings in the background.\n",
      "hypo: in this image we can see a group of people standing on the ground. in their hands. in the background there is a flag.\n",
      "Val epoch = 8, WER = 76.35205078125, BLEU = 31.176040649414062\n",
      "refe: on the left side of the image we can see a person is standing near a bench. in the middle of the image we can see some clothes are hung to a hunger and two persons are there. on the right side of the image we can see a person and some clothes hung to a hunger.\n",
      "hypo: in this image we can see a person standing on the ground. on the right side of the right side of the background we can see the ground.\n",
      "Val epoch = 8, WER = 76.20578002929688, BLEU = 31.18468475341797\n",
      "refe: in the image we can see there is a painting on the wall of a dog standing and there is matter written on the wall. the wall is made up of stone bricks.\n",
      "hypo: in this image we can see a poster on the wall. on the wall.\n",
      "Val epoch = 8, WER = 76.24208068847656, BLEU = 31.16986846923828\n",
      "refe: in front of the image there is a wooden log with a tool on it. beside that there is a machine. and also there are wooden poles with equipment. and there is a person holding the wooden object. and there are few people in the image. on the left corner of the image there is a table with fire. and in the background there are buildings, pole with a sign board, railings, walls, water and chain. on the ground there is a box,\n",
      "hypo: in this image we can see a person wearing black color t - shirt, trouser, trouser, we can see some objects.\n",
      "Validation WER: 76.24208385810076\n",
      "Validation BLEU: 31.169866611934026\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb590201ddec4d5db7df010550a99098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12687 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 2.2220470647994886e-08\n",
      "Train epoch = 9, loss = 164.63665771484375, WER = 82.31907653808594, BLEU = 25.356525421142578\n",
      "refe: there are few persons playing on the ground. this is grass and there is a ball. in the background we can see a mesh.\n",
      "hypo: in this image i can see playing playing playing playing the the...... the\n",
      "lr: 1.851764273140485e-08\n",
      "Train epoch = 9, loss = 178.0460205078125, WER = 81.39530944824219, BLEU = 23.11852264404297\n",
      "refe: this picture shows a couple of men standing and holding a wine bottles in their hand and we see people seated on the chairs\n",
      "hypo: in this image standing standing a holding. bottle holding holding holding holding bottle bottle bottle bottle the and bottle and and and and\n",
      "lr: 1.4814814814814813e-08\n",
      "Train epoch = 9, loss = 175.54296875, WER = 81.08969116210938, BLEU = 23.35457992553711\n",
      "refe: in this image i can see a man is sitting and smiling. this picture is black and white in color.\n",
      "hypo: in this image black and white image. a sitting a a a. a a\n",
      "lr: 1.1111986898224779e-08\n",
      "Train epoch = 9, loss = 180.14634704589844, WER = 81.0849609375, BLEU = 23.311595916748047\n",
      "refe: in this image there are two dogs sitting on the ground. there is grass on the ground. behind them there are hedges and plants. there are flowers to the plants. in the background there are trees. at the top there is the sky. there is text above the head of the dogs. below the picture there is the text. in the bottom right there are numbers on the image. this image seems to be a poster.\n",
      "hypo: in this image we can see two two two two two two the two the the the.... the. the\n",
      "lr: 7.409158981634745e-09\n",
      "Train epoch = 9, loss = 173.0723419189453, WER = 81.0543212890625, BLEU = 23.02349090576172\n",
      "refe: there is a woman standing in the center. she is holding a glass of wine in her left hand and flowers in her left hand and she is smiling.\n",
      "hypo: in this image black and white image woman a holding holding a a holding holding holding. holding bouquet.. the. the the\n",
      "lr: 3.7063310650447087e-09\n",
      "Train epoch = 9, loss = 184.73696899414062, WER = 81.16264343261719, BLEU = 23.113872528076172\n",
      "refe: in this image we can see a person wearing the helmet and holding the baseball bat. we can also see the ball and also the sand in the background.\n",
      "hypo: in this image i can a a a a standing the the bat bat bat bat bat bat bat bat bat\n",
      "lr: 3.5031484546736375e-12\n",
      "Train epoch = 9, loss = 169.8361358642578, WER = 81.18677520751953, BLEU = 23.0801944732666\n",
      "refe: in this image there is a ship. in the bottom left there are people standing on the ship. in front of them there is the water. behind the ship there is the water.\n",
      "hypo: in this image there can see a ship ship ship ship and and ship the. ship.\n",
      "Train epoch = 9, loss = 170.46163940429688, WER = 81.2136459350586, BLEU = 23.121442794799805\n",
      "refe: in this image, we can see few peoples are sat on the black color chair. we can see wooden tables in the middle. so many items are placed on it. at the back side, we can see few peoples are standing. near the screen, we can see banners. and white color pillars on the left side and white color wall. the left side, we can see cream color curtains.\n",
      "hypo: in this image i sitting see there sitting sitting on chairs. chairs.............\n",
      "学習率: 0.0\n",
      "Train loss: 170.46162246704102\n",
      "Train crf: 75.58207675933838\n",
      "Train ca: 94.87954582214356\n",
      "Train WER: 81.21364000340142\n",
      "Train BLEU: 23.12144130397844\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34fc89486e7495bb53a13e31af893ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val epoch = 9, WER = 75.74626922607422, BLEU = 28.057846069335938\n",
      "refe: in this image i can see few trees which are green in color, few flowers which are red in color and in the background i can see a person standing, the road, few vehicles, few buildings, few trees and the sky.\n",
      "hypo: in this image we can see plants, plants, we can see the background we can see the background there are buildings.\n",
      "Val epoch = 9, WER = 76.31013488769531, BLEU = 31.327890396118164\n",
      "refe: in this picture, we see men in the uniform are standing. the man on the left side is holding a wooden stick. behind him, we see a man is holding a green color flag. we see people are holding flags which are in red, blue and green color. in the background, we see a car and buildings. there are poles and buildings in the background.\n",
      "hypo: in this image we can see a group of people standing on the ground. in their hands. in the background there is a flag.\n",
      "Val epoch = 9, WER = 76.43274688720703, BLEU = 31.079444885253906\n",
      "refe: on the left side of the image we can see a person is standing near a bench. in the middle of the image we can see some clothes are hung to a hunger and two persons are there. on the right side of the image we can see a person and some clothes hung to a hunger.\n",
      "hypo: in this image we can see a person standing on the road. in front of the right side of the right side of the background we can see the background there is a road.\n",
      "Val epoch = 9, WER = 76.34611511230469, BLEU = 31.072622299194336\n",
      "refe: in the image we can see there is a painting on the wall of a dog standing and there is matter written on the wall. the wall is made up of stone bricks.\n",
      "hypo: in this image we can see a poster on the wall. on the wall.\n",
      "Val epoch = 9, WER = 76.37439727783203, BLEU = 31.057226181030273\n",
      "refe: in front of the image there is a wooden log with a tool on it. beside that there is a machine. and also there are wooden poles with equipment. and there is a person holding the wooden object. and there are few people in the image. on the left corner of the image there is a table with fire. and in the background there are buildings, pole with a sign board, railings, walls, water and chain. on the ground there is a box,\n",
      "hypo: in this image we can see a group of people standing, trouser, trouser, trouser, we can see the background we can see some objects.\n",
      "Validation WER: 76.37439605499362\n",
      "Validation BLEU: 31.057225603317097\n"
     ]
    }
   ],
   "source": [
    "config = ConfigTrain()\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path = config.bert_model_path)\n",
    "#model_id = \"google-bert/bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(config.model_id)\n",
    "\n",
    "# 辞書サイズを保存\n",
    "vocab_size = len( tokenizer )\n",
    "\n",
    "# モデル出力用のディレクトリを作成\n",
    "os.makedirs(config.save_directory, exist_ok=True)\n",
    "\n",
    "# 画像のtransformsを定義\n",
    "transforms = v2.Compose([\n",
    "    v2.Resize((336, 336)),\n",
    "    v2.AutoAugment(),\n",
    "    #v2.ToTensor(),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    ## Coco データセット 2017 train の平均と標準偏差\n",
    "    #v2.Normalize((0.456,0.427,0.401),(0.224,0.219,0.231) )\n",
    "    # ImageNetデータセットの平均と標準偏差\n",
    "    #v2.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    # Clip Model の config から引用。\n",
    "    v2.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "# v7 データセット\n",
    "train_dataset = MyDataset( file_path=config.anno_file,\n",
    "                           img_directory = config.img_directory,\n",
    "                           transforms=transforms,tokenizer=tokenizer, length_max = config.length_max)\n",
    "\n",
    "# Subset samplerの生成\n",
    "test_set, val_set, train_set = util.generate_subset_test_val_train(\n",
    "    train_dataset, config.test_ratio, config.val_ratio )\n",
    "    \n",
    "# 学習時にランダムにサンプルするためのサンプラー\n",
    "train_sampler = SubsetRandomSampler(train_set)\n",
    "\n",
    "# DataLoaderを生成\n",
    "collate_func_lambda = lambda x: collate_func(x, tokenizer.pad_token_id, config.length_max)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=config.batch_size,\n",
    "                    num_workers=config.num_workers,\n",
    "                    sampler=train_sampler,\n",
    "                    collate_fn=collate_func_lambda)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=config.batch_size,\n",
    "                    num_workers=config.num_workers,\n",
    "                    sampler=val_set,\n",
    "                    collate_fn=collate_func_lambda)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    #batch_size=config.batch_size,\n",
    "                    batch_size=1,\n",
    "                    num_workers=config.num_workers,\n",
    "                    sampler=test_set,\n",
    "                    collate_fn=collate_func_lambda)\n",
    "\n",
    "\n",
    "print( \"config.device:\", config.device )\n",
    "print( \"学習セット数:\",len( train_loader ) )\n",
    "print( \"評価セット数:\",len( val_loader ))\n",
    "print( \"テストセット数:\",len( test_loader ))\n",
    "print( \"use_amp:\", config.use_amp )\n",
    "print( \"use_saved_pth:\", config.use_saved_pth )\n",
    "\n",
    "# モデルの定義\n",
    "model = CaptioningTransformer( config.img_size,\n",
    "    config.dim_embedding, config.length_max, config.vocab_size,\n",
    "    tokenizer, config.dropout, config.model_id)\n",
    "model.to(config.device)\n",
    "#crf_low_rank = 32\n",
    "#crf_beam_size = 256\n",
    "#top_dropout = 0.0\n",
    "#tgt_padding_idx = tokenizer.pad_token_id\n",
    "#toplayer = TopLayer( vocab_size, dim_embedding, crf_low_rank, crf_beam_size, top_dropout, tgt_padding_idx )\n",
    "\n",
    "\n",
    "# 損失関数の定義\n",
    "#criterion = nn.CrossEntropyLoss( ignore_index = tokenizer.pad_token_id, reduction = 'mean' )\n",
    "#criterion = nn.CrossEntropyLoss( reduction = 'mean' )\n",
    "log_softmax = nn.LogSoftmax( dim = 2 )\n",
    "#softmax = nn.Softmax( dim = 2 )\n",
    "criterion_nll = nn.NLLLoss( reduction = 'none' )\n",
    "#criterion_nll = nn.NLLLoss( )\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 最適化手法の定義\n",
    "# 最適化手法の定義\n",
    "# Optimizerの生成, clipとそうでないモジュールとの\n",
    "# パラメータで異なる学習率を適用\n",
    "params_clip = []\n",
    "params_bert = []\n",
    "params_others = []\n",
    "for name, parameter in model.named_parameters():\n",
    "    if parameter.requires_grad:\n",
    "        if 'clip_model' in name:\n",
    "            params_clip.append(parameter)\n",
    "        elif 'bert' in name:\n",
    "            params_bert.append(parameter)\n",
    "        else:\n",
    "            params_others.append(parameter)\n",
    "param_groups = [\n",
    "    {'params': params_clip, 'lr': config.lr_clip},\n",
    "    {'params': params_bert, 'lr': config.lr_bert},\n",
    "    {'params': params_others, 'lr': config.lr_others}]\n",
    "#optimizer = torch.optim.AdamW( model.parameters() , lr=config.lr)\n",
    "#optimizer = torch.optim.AdamW( param_groups, weight_decay = config.weight_decay, betas=config.betas )\n",
    "optimizer = torch.optim.AdamW( param_groups, weight_decay = config.weight_decay, betas=config.betas )\n",
    "#t_optimizer = torch.optim.AdamW( toplayer.parameters(), lr = config.lr_top, weight_decay = config.weight_decay, betas=config.betas )\n",
    "\n",
    "\n",
    "# 全ステップ数\n",
    "num_global_steps = len( train_loader ) * config.num_epochs\n",
    "print( \"num_global_steps:\", num_global_steps )\n",
    "num_warmup_steps = num_global_steps * config.warmup\n",
    "print( \"num_warmup_steps:\", num_warmup_steps )\n",
    "#スケジューラーの定義\n",
    "scheduler = get_linear_schedule_with_warmup( optimizer, num_warmup_steps, num_global_steps )    \n",
    "#t_scheduler = get_linear_schedule_with_warmup( t_optimizer, num_warmup_steps, num_global_steps )    \n",
    "\n",
    "\n",
    "PATH = \"model/model_bert_large_NAR_PAD_curr.pth\"\n",
    "print( \"use_saved_pth:\", config.use_saved_pth )\n",
    "print( \"exist saved_pth:\", os.path.isfile(PATH) ) \n",
    "use_saved_pth = config.use_saved_pth\n",
    "if use_saved_pth and os.path.isfile(PATH):\n",
    "    checkpoint = torch.load(PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ## optimizerのstateを現在のdeviceに移す。これをしないと、保存前後でdeviceの不整合が起こる可能性がある。\n",
    "    #for state in optimizer.state.values():\n",
    "        #for k, v in state.items():\n",
    "            #if isinstance(v, torch.Tensor):\n",
    "                #state[k] = v.to(device)\n",
    "    begin_epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    global_step = checkpoint['global_step']    \n",
    "else:\n",
    "    begin_epoch = 0\n",
    "    global_step = 0\n",
    "\n",
    "print( \"begin_epoch:\", begin_epoch )\n",
    "print( \"global_ste:\", global_step )\n",
    "\n",
    "len_tr_loader = len( train_loader )\n",
    "train_param = len_tr_loader // 6\n",
    "#train_param = len_tr_loader // 100\n",
    "len_val_loader = len( val_loader )\n",
    "#train_param = len_val_loader // 3\n",
    "val_param = len_val_loader // 3\n",
    "print( \"train_param:\", train_param )\n",
    "print( \"val_param:\", val_param )\n",
    "\n",
    "print( \"epochs:\", config.num_epochs )\n",
    "print( \"batch_size:\", config.batch_size )\n",
    "print( \"lr_clip:\", config.lr_clip )\n",
    "print( \"lr_bert:\", config.lr_bert )\n",
    "print( \"lr_others:\", config.lr_others )\n",
    "#print( \"lr_top   :\", config.lr_top )\n",
    "print( \"weight_decay:\", config.weight_decay )\n",
    "print( \"betas:\", config.betas )\n",
    "\n",
    "# 学習経過の書き込み\n",
    "now = datetime.datetime.now()\n",
    "train_loss_file = '{}/MyOriginal_train_loss_{}.csv'\\\n",
    "    .format(config.save_directory, now.strftime('%Y%m%d_%H%M%S'))\n",
    "with open(train_loss_file, 'a') as f:\n",
    "    print(f'{len_tr_loader}', file=f) \n",
    "print( \"train_loss_file:\", train_loss_file )\n",
    "val_loss_file = '{}/MyOriginal_val_loss_{}.csv'\\\n",
    "    .format(config.save_directory, now.strftime('%Y%m%d_%H%M%S'))\n",
    "with open(val_loss_file, 'a') as f:\n",
    "    print(f'{len_val_loader}', file=f) \n",
    "norm_file = '{}/norm_{}.csv'\\\n",
    "    .format(config.save_directory, now.strftime('%Y%m%d_%H%M%S'))\n",
    "\n",
    "# 学習\n",
    "val_loss_best = float('inf')\n",
    "\n",
    "fn = bleu_score.SmoothingFunction().method7\n",
    "\n",
    "# AMP用のスケーラー\n",
    "scaler = GradScaler(enabled=config.use_amp)\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    with tqdm(train_loader) as pbar:\n",
    "    #with tqdm(val_loader) as pbar:\n",
    "        pbar.set_description(f'[エポック {epoch + 1}]')\n",
    "\n",
    "        # 学習モードに設定\n",
    "        model.train()\n",
    "\n",
    "        train_losses = deque()\n",
    "        train_a = deque()\n",
    "        train_b = deque()\n",
    "        train_errors = deque()\n",
    "        train_bleus = deque()\n",
    "        for n_batch, (imgs, captions, caption_lengths) in enumerate( pbar ):\n",
    "            # ミニバッチを設定\n",
    "            imgs = imgs.to(config.device)\n",
    "            captions = captions.to(config.device)\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 最後の単語から次を予測する必要はないため最後の単語を除外\n",
    "            with autocast(str(config.device),enabled=config.use_amp):\n",
    "                outputs = model( imgs )\n",
    "\n",
    "                # 損失の計算\n",
    "                # 単語軸が第1軸である必要があるため、転置\n",
    "                #outputs = outputs.transpose(1, 2)\n",
    "                src_representation = outputs\n",
    "                #src_input = imgs\n",
    "                src_input = outputs\n",
    "                tgt_input = captions\n",
    "                train_batch_crf_loss = model.toplayer( src_representation, src_input, tgt_input, is_training = True )\n",
    "                #a = torch.mean( train_batch_crf_loss ) / 200\n",
    "                a = torch.mean( train_batch_crf_loss ) # mean over bsz\n",
    "                log_probs = log_softmax( outputs ).transpose(0,1)\n",
    "                ##log_probs = log_softmax( outputs.transpose(0,1) )\n",
    "                ##bsz, seq = captions.size()\n",
    "                train_nll_loss = calc_loss_nll( log_probs, captions )\n",
    "                b = config.alpha * torch.mean( train_nll_loss )\n",
    "                #b = config.alpha * train_nll_loss\n",
    "                #b = torch.tensor( 0 )\n",
    "                #train_nll_loss = calc_loss_ca( outputs, captions, config.window_size )\n",
    "                #loss_ca = calc_loss_ca( outputs, captions, config.window_size )\n",
    "                #b = config.alpha * loss_ca\n",
    "                loss = a + b\n",
    "                #loss = a \n",
    "            \n",
    "                #loss = loss_CRF + config.alpha * loss_CA\n",
    "\n",
    "            hypo_ids = torch.argmax( outputs, dim = 2 )\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            #scaler.unscale_(optimizer)\n",
    "            #clip_grad_threshold = 1.0\n",
    "            #torch.nn.utils.clip_grad_norm_(\\\n",
    "            #        model.parameters(),\n",
    "            #        clip_grad_threshold)\n",
    "            # オプティマイザにより，パラメータを更新する\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()            \n",
    "            scheduler.step()\n",
    "\n",
    "            #for name, param in model.named_parameters():\n",
    "            #    print( name )\n",
    "            \n",
    "            norm0 = torch.sqrt( torch.norm( model.clip_model.vision_model.encoder.layers[0].self_attn.q_proj.weight.grad, p = 2 ) ).item()\n",
    "            norm1 = torch.sqrt( torch.norm( model.bert.encoder.layer[23].attention.self.query.weight.grad, p = 2 ) ).item()\n",
    "            norm_mean = torch.mean( torch.stack ([ torch.sqrt( torch.norm( param.grad, p = 2 ) ) \\\n",
    "                                                  for param in model.parameters() if param.grad is not None ] ) ).item()\n",
    "            with open(norm_file, 'a') as f:\n",
    "                print( \"epcoch:\", epoch, \", step:\", global_step, \", norm0:\", norm0, \", norm1:\", norm1, \", norm_mean:\", norm_mean, file=f  )\n",
    "                f.flush()\n",
    "            global_step += 1\n",
    "\n",
    "            n = 0\n",
    "            hypo_sentence = []\n",
    "            ref_sentence = []\n",
    "            hypo_sentence1 = []\n",
    "            ref_sentence1 = []\n",
    "            total_error = 0\n",
    "            total_token_length = 0\n",
    "            total_bleu = 0\n",
    "            n2 = 0\n",
    "            for (hypo_id, caption) in zip( hypo_ids, captions ):\n",
    "                #hypo = tokenizer.decode( hypo_id.tolist(), skip_special_tokens = True )\n",
    "                hypo = model.my_decode( hypo_id.tolist(), tokenizer )\n",
    "                hypo_tokens = tokenizer.tokenize( hypo )\n",
    "                #reference = tokenizer.decode( caption.tolist(), skip_special_tokens = True )\n",
    "                reference = model.my_decode( caption.tolist(), tokenizer )\n",
    "                ref_tokens = tokenizer.tokenize( reference )\n",
    "                        \n",
    "                # 認識誤りを計算\n",
    "                (error, substitute, \n",
    "                    delete, insert, ref_length) = \\\n",
    "                    levenshtein.calculate_error(hypo_tokens,\n",
    "                                                    ref_tokens)\n",
    "                \n",
    "                # 誤り文字数を累積する\n",
    "                total_error += error\n",
    "                # 文字の総数を累積する\n",
    "                total_token_length += ref_length\n",
    "\n",
    "                bleu = bleu_score.sentence_bleu( [reference], hypo, smoothing_function=fn  )\n",
    "        \n",
    "                total_bleu += bleu                    \n",
    "                    \n",
    "                if n < 1 and n_batch == len( train_loader ) - 1 :\n",
    "                    hypo_sentence.append( hypo )\n",
    "                    ref_sentence.append( reference )\n",
    "                if n < 1 and n_batch % train_param == 0:\n",
    "                    hypo_sentence1.append( hypo )\n",
    "                    ref_sentence1.append( reference )\n",
    "                    \n",
    "                n += 1\n",
    "                n2 += 1\n",
    "            \n",
    "            avg_error = total_error / total_token_length * 100\n",
    "            avg_bleu = total_bleu / n2 * 100\n",
    "                \n",
    "            # 学習時の損失をログに書き込み\n",
    "            train_losses.append(loss.item())\n",
    "            train_a.append(a.item())\n",
    "            train_b.append(b.item())\n",
    "            train_errors.append( avg_error )\n",
    "            train_bleus.append( avg_bleu )\n",
    "            #train_ciders.append( avg_cider )\n",
    "            if len(train_losses) > config.moving_avg:\n",
    "                train_losses.popleft()\n",
    "                train_a.popleft()\n",
    "                train_b.popleft()\n",
    "                train_errors.popleft()\n",
    "                train_bleus.popleft()\n",
    "                #train_ciders.popleft()\n",
    "            mean_loss = torch.Tensor(train_losses).mean().item()\n",
    "            mean_a = torch.Tensor(train_a).mean().item()\n",
    "            mean_b = torch.Tensor(train_b).mean().item()\n",
    "            mean_error = torch.Tensor(train_errors).mean().item()\n",
    "            mean_bleu = torch.Tensor(train_bleus).mean().item()\n",
    "            pbar.set_postfix({\n",
    "                'loss': mean_loss,\n",
    "                'crf': mean_a,\n",
    "                'ca': mean_b,\n",
    "                'WER': mean_error,\n",
    "                'BLEU': mean_bleu,\n",
    "                #'CIDER': torch.Tensor(train_ciders).mean().item()\n",
    "            })\n",
    "            with open(train_loss_file, 'a') as f:\n",
    "                print(f'{epoch}, {mean_loss}, {mean_a}, {mean_b}, {mean_error}, {mean_bleu}', file=f)\n",
    "            print_flag = 1\n",
    "            for ( hypo_se, ref_se ) in zip( hypo_sentence1, ref_sentence1 ):\n",
    "                if print_flag == 1:\n",
    "                    print( \"lr:\", optimizer.param_groups[0][\"lr\"] )\n",
    "                    print_flag = 0\n",
    "                print(f'Train epoch = {epoch}, loss = {mean_loss}, WER = {mean_error}, BLEU = {mean_bleu}')\n",
    "                print( \"refe:\", ref_se )\n",
    "                print( \"hypo:\", hypo_se )\n",
    "                    \n",
    "            for ( hypo_se, ref_se ) in zip( hypo_sentence, ref_sentence ):\n",
    "                print(f'Train epoch = {epoch}, loss = {mean_loss}, WER = {mean_error}, BLEU = {mean_bleu}')\n",
    "                print( \"refe:\", ref_se )\n",
    "                print( \"hypo:\", hypo_se )\n",
    "    # 学習率を表示\n",
    "    print(f'学習率: {optimizer.param_groups[0]['lr']}')\n",
    "    train_loss = np.mean(train_losses)\n",
    "    train_a1 = np.mean(train_a)\n",
    "    train_b1 = np.mean(train_b)\n",
    "    train_error = np.mean(train_errors )\n",
    "    train_bleu = np.mean(train_bleus )\n",
    "    print(f'Train loss: {train_loss}')\n",
    "    print(f'Train crf: {train_a1}')\n",
    "    print(f'Train ca: {train_b1}')\n",
    "    print(f'Train WER: {train_error}')        \n",
    "    print(f'Train BLEU: {train_bleu}')\n",
    "\n",
    "    # 検証\n",
    "    with tqdm(val_loader) as pbar:\n",
    "        pbar.set_description(f'[検証]')\n",
    "\n",
    "        # 評価モード\n",
    "        model.eval()\n",
    "\n",
    "        #val_losses = []\n",
    "        #val_losses = deque()\n",
    "        #val_a = deque()\n",
    "        #val_b = deque()\n",
    "        val_errors = deque()\n",
    "        val_bleus = deque()\n",
    "        for n_batch, (imgs, captions, caption_lengths) in enumerate( pbar ):\n",
    "\n",
    "            # ミニバッチを設定\n",
    "            imgs = imgs.to(config.device)\n",
    "            captions = captions.to(config.device)\n",
    "            #caption_lengths = torch.tensor( caption_lengths ).to(config.device)\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                #outputs = model( imgs )\n",
    "                finalized_tokens = model.inference( imgs )\n",
    "                #hypo_ids = torch.argmax( outputs, dim = 2 )\n",
    "                hypo_ids = finalized_tokens\n",
    "                ## 損失の計算\n",
    "                ## 単語軸が第1軸である必要があるため、転置\n",
    "                ##outputs = outputs.transpose(1, 2)\n",
    "                #src_representation = outputs\n",
    "                ##src_input = imgs\n",
    "                #src_input = outputs\n",
    "                #tgt_input = captions\n",
    "                #train_batch_crf_loss = model.toplayer( src_representation, src_input, tgt_input, is_training = True )\n",
    "                #log_probs = log_softmax( outputs.transpose(0,1) )\n",
    "                ##bsz, seq = captions.size()\n",
    "                ##loss_CA = - criterion_nll( log_prob.view( bsz * seq, -1 ) , captions.view( bsz * seq ) )\n",
    "                #train_nll_loss = calc_loss_ca( log_probs, captions )\n",
    "                #a = torch.mean( train_batch_crf_loss )\n",
    "                #b = config.alpha * torch.mean( train_nll_loss )\n",
    "                #loss = a + b\n",
    "               \n",
    "            n = 0\n",
    "            hypo_sentence = []\n",
    "            ref_sentence = []\n",
    "            hypo_sentence1 = []\n",
    "            ref_sentence1 = []\n",
    "            total_error = 0\n",
    "            total_token_length = 0\n",
    "            total_bleu = 0\n",
    "            n2 = 0\n",
    "            for (hypo_id, caption) in zip( hypo_ids, captions ):\n",
    "                #hypo = tokenizer.decode( hypo_id.tolist(), skip_special_tokens = True )\n",
    "                hypo = model.my_decode( hypo_id.tolist(), tokenizer )\n",
    "                hypo_tokens = tokenizer.tokenize( hypo )\n",
    "                #reference = tokenizer.decode( caption.tolist(), skip_special_tokens = True )\n",
    "                reference = model.my_decode( caption.tolist(), tokenizer )\n",
    "                ref_tokens = tokenizer.tokenize( reference )\n",
    "\n",
    "                        \n",
    "                # 認識誤りを計算\n",
    "                (error, substitute, \n",
    "                    delete, insert, ref_length) = \\\n",
    "                    levenshtein.calculate_error(hypo_tokens,\n",
    "                                                ref_tokens)\n",
    "                    \n",
    "                # 誤り文字数を累積する\n",
    "                total_error += error\n",
    "                # 文字の総数を累積する\n",
    "                total_token_length += ref_length\n",
    "\n",
    "                bleu = bleu_score.sentence_bleu( [reference], hypo, smoothing_function=fn  )\n",
    "        \n",
    "                total_bleu += bleu\n",
    "\n",
    "                if n < 1 and n_batch == len( val_loader ) - 1:\n",
    "                    hypo_sentence.append( hypo )\n",
    "                    ref_sentence.append( reference )\n",
    "                        \n",
    "                if n < 1 and n_batch % val_param == 0:\n",
    "                    hypo_sentence1.append( hypo )\n",
    "                    ref_sentence1.append( reference )\n",
    "                    \n",
    "                n += 1\n",
    "                n2 += 1\n",
    "                \n",
    "            avg_error = total_error / total_token_length * 100                    \n",
    "            avg_bleu = total_bleu / n2 * 100\n",
    "\n",
    "            # 学習時の損失をログに書き込み\n",
    "            #val_losses.append(loss.item())\n",
    "            #val_a.append(a.item())\n",
    "            #val_b.append(b.item())\n",
    "            val_errors.append( avg_error )\n",
    "            val_bleus.append( avg_bleu )\n",
    "            if len(val_errors) > config.moving_avg:\n",
    "                #val_losses.popleft()\n",
    "                #val_a.popleft()\n",
    "                #val_b.popleft()\n",
    "                val_errors.popleft()\n",
    "                val_bleus.popleft()\n",
    "            #mean_loss = torch.Tensor(val_losses).mean().item()\n",
    "            #mean_a = torch.Tensor(val_a).mean().item()\n",
    "            #mean_b = torch.Tensor(val_b).mean().item()\n",
    "            mean_error = torch.Tensor(val_errors).mean().item()\n",
    "            mean_bleu = torch.Tensor(val_bleus).mean().item()\n",
    "            pbar.set_postfix({\n",
    "                #'loss': mean_loss,\n",
    "                #'crf': mean_a,\n",
    "                #'ca': mean_b,\n",
    "                'WER': mean_error,\n",
    "                'BLEU': mean_bleu,\n",
    "            })\n",
    "            # Validation Lossをログに書き込み\n",
    "            with open(val_loss_file, 'a') as f:\n",
    "                print(f'{epoch}, {mean_error}, {mean_bleu}', file=f)\n",
    "            \n",
    "            for ( hypo_se, ref_se ) in zip( hypo_sentence1, ref_sentence1 ):\n",
    "                print(f'Val epoch = {epoch}, WER = {mean_error}, BLEU = {mean_bleu}')\n",
    "                print( \"refe:\", ref_se )\n",
    "                print( \"hypo:\", hypo_se )\n",
    "                    \n",
    "            for ( hypo_se, ref_se ) in zip( hypo_sentence, ref_sentence ):\n",
    "                print(f'Val epoch = {epoch}, WER = {mean_error}, BLEU = {mean_bleu}')\n",
    "                print( \"refe:\", ref_se )\n",
    "                print( \"hypo:\", hypo_se )\n",
    "                    \n",
    "    # Loss 表示\n",
    "    #val_loss = np.mean(val_losses)\n",
    "    val_error = np.mean( val_errors )\n",
    "    val_bleu = np.mean( val_bleus )\n",
    "    #print(f'Validation loss: {val_loss}')\n",
    "    print(f'Validation WER: {val_error}')\n",
    "    print(f'Validation BLEU: {val_bleu}')\n",
    "\n",
    "    ## より良い検証結果が得られた場合、モデルを保存\n",
    "            \n",
    "    # モデルを保存\n",
    "    torch.save({'epoch': epoch,\n",
    "                'global_step': global_step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'loss': loss,},\n",
    "        f'{config.save_directory}/model_bert_large_NAR_PAD_curr.pth')\n",
    "    ## モデルを保存\n",
    "        \n",
    "# モデルを保存\n",
    "torch.save({'epoch': epoch,\n",
    "    'global_step': global_step,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'loss': loss,},\n",
    "    f'{config.save_directory}/model_bert_large_NAR_PAD_final.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_ca( logits, captions, c ):\n",
    "\n",
    "    eps = 1e-4\n",
    "\n",
    "    B, T, V = logits.size()\n",
    "    \n",
    "    one_hot_cap = F.one_hot( captions, num_classes = len( tokenizer ) ) # B * T * V\n",
    "\n",
    "    lcabi = torch.zeros( (B, T),  dtype=torch.float, device = logits.device )\n",
    "    zeroB = torch.zeros( (B),  dtype=torch.float, device = logits.device )\n",
    "    for i in range( T ):\n",
    "        tmp = torch.stack( [ torch.log( (  1.0 - torch.exp( torch.sum( logits[:,j,:] * one_hot_cap[:,i,:], dim = 1 ) ) / \\\n",
    "            ( torch.sum( torch.exp(logits[:,j]), dim = 1 ) + eps ) + eps ) ) if j != i  else zeroB \\\n",
    "            for j in range( max( 0, i - c ), min(  T, i + c ) ) ], dim = 0 )  # window 幅 * B\n",
    "        lcabi[:,i] = torch.sum( tmp, dim = 0 ) # wubdiow 幅 * B を window 幅について sum\n",
    "    \n",
    "    # lcabi は B * T\n",
    "\n",
    "    lca = torch.sum( torch.sum( lcabi , dim = 1 ), dim = 0 )\n",
    "    \n",
    "    return lca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cnt_repeat( logits, c, tau ):\n",
    "\n",
    "    B, T, V = logits.size()\n",
    "    \n",
    "    def differentiable_argamx( logits, tau ):\n",
    "\n",
    "        tmp = F.gumbel_softmax( logits, tau, hard=True )\n",
    "        tmp1 = torch.arange( 0, logits.size(2) )[None,None] * tmp\n",
    "        tokens = torch.sum( tmp1, dim = 2 )\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    tokens = differentiable_argamx( logits, tau ) #logits から token を算出。微分可能 B * T\n",
    "\n",
    "    cnt = 0\n",
    "    for i in range( T ):\n",
    "        cntj = 0\n",
    "        for j in range( max( 0, i - c ), min(  T, i + c ) ):\n",
    "            if j != i:\n",
    "                tmp = torch.abs( tokens[:,j] - tokens[:,i] ) # i と j が同じだったら0 その他は 1 以上の整数 B\n",
    "                tmp = F.sigmoid( 10 - 100 * tmp ) # 同じところだけ 1, あとは 0. B\n",
    "                cntj += torch.sum( tmp ) # B についての sum　を j　について足しこんでいる。\n",
    "        cnt += cntj # i について足しこんでいる。\n",
    "\n",
    "    return cnt"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "b411586366dd8bdb0fc2800d6aef8fbd6cc14d2a48eec1b457375028a8621915"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "10ac0991cf0d4346a2dd950c4e69de74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc134416b78b4e16ab143f7b3c9b4db1",
      "placeholder": "​",
      "style": "IPY_MODEL_3e3ad9c31b8244d9987cb5d09ac80909",
      "value": " 230M/230M [00:03&lt;00:00, 57.2MB/s]"
     }
    },
    "189ddc1a5e674235a5a4c9461ec37bb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89ed549065394bad9c015c612b04c1d5",
      "placeholder": "​",
      "style": "IPY_MODEL_f9994c1870c348b0ab983404decb722e",
      "value": "100%"
     }
    },
    "2030796b20a24135908446e8a05b2447": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e3ad9c31b8244d9987cb5d09ac80909": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "741ba7eec1f94e6889152972dbf220c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89ed549065394bad9c015c612b04c1d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af8d4c88aa714714b864351862297512": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_741ba7eec1f94e6889152972dbf220c0",
      "max": 241669177,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_be3b477f459a4fdd80b055ef88cf8b52",
      "value": 241669177
     }
    },
    "be3b477f459a4fdd80b055ef88cf8b52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dc134416b78b4e16ab143f7b3c9b4db1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee1785c740b7449584ceec0225c64c78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_189ddc1a5e674235a5a4c9461ec37bb7",
       "IPY_MODEL_af8d4c88aa714714b864351862297512",
       "IPY_MODEL_10ac0991cf0d4346a2dd950c4e69de74"
      ],
      "layout": "IPY_MODEL_2030796b20a24135908446e8a05b2447"
     }
    },
    "f9994c1870c348b0ab983404decb722e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
